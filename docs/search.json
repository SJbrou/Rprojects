[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R projects",
    "section": "",
    "text": "A collection of my projects in R.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Home</span>"
    ]
  },
  {
    "objectID": "Manuscripts/Environmental impact/index.html",
    "href": "Manuscripts/Environmental impact/index.html",
    "title": "Environmental impact of utilities",
    "section": "",
    "text": "Utility consumption\nAssuming the following usage pattern:\nFigure 2.1: Assumed utility usage. Note that energy meters measure net flow, and thus gross solar and power might differ.",
    "crumbs": [
      "Personal",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environmental impact of home insulation</span>"
    ]
  },
  {
    "objectID": "Manuscripts/Environmental impact/index.html#utility-consumption",
    "href": "Manuscripts/Environmental impact/index.html#utility-consumption",
    "title": "Environmental impact of utilities",
    "section": "",
    "text": "Details\n\n\n\n\n\n\nDate\nGas (m³)\nElectricity Usage (kWh)\nSolar (kWh)\n\n\n\n\n2025-01-01\n126.0\n180.0\n40.6\n\n\n2024-12-01\n107.0\n202.0\n15.4\n\n\n2024-11-01\n86.9\n201.0\n58.4\n\n\n2024-10-01\n33.8\n153.0\n123.0\n\n\n2024-09-01\n8.2\n120.0\n191.0\n\n\n2024-08-01\n8.1\n102.0\n286.0\n\n\n2024-07-01\n9.2\n91.5\n292.0\n\n\n2024-06-01\n9.7\n115.0\n264.0\n\n\n2024-05-01\n7.9\n131.0\n300.0\n\n\n2024-04-01\n42.8\n141.0\n209.0\n\n\n2024-03-01\n60.9\n147.0\n155.0\n\n\n2024-02-01\n116.0\n236.0\n77.0\n\n\n2024-01-01\n162.0\n258.0\n32.0\n\n\n2023-12-01\n144.0\n232.0\n38.0\n\n\n2023-11-01\n41.0\n182.0\n67.0\n\n\n2023-10-01\n14.0\n124.0\n113.0\n\n\n2023-09-01\n7.0\n72.0\n243.0\n\n\n2023-08-01\n10.0\n57.0\n286.0\n\n\n2023-07-01\n17.0\n126.0\n266.0\n\n\n2023-06-01\n20.0\n101.0\n364.0\n\n\n2023-05-01\n33.0\n116.0\n320.0\n\n\n2023-04-01\n77.0\n161.0\n232.0\n\n\n2023-03-01\n124.0\n216.0\n135.0\n\n\n2023-02-01\n116.0\n236.0\n77.0\n\n\n2023-01-01\n162.0\n258.0\n32.0\n\n\n2022-12-01\n130.0\n232.0\n38.0\n\n\n2022-11-01\n41.0\n182.0\n67.0\n\n\n2022-10-01\n24.0\n197.0\n120.0\n\n\n2022-09-01\n20.0\n158.0\n195.0\n\n\n2022-08-01\n12.0\n136.0\n319.0\n\n\n2022-07-01\n17.0\n109.0\n301.0\n\n\n2022-06-01\n24.0\n130.0\n310.0\n\n\n2022-05-01\n34.0\n116.0\n325.0\n\n\n2022-04-01\n65.0\n136.0\n276.0\n\n\n2022-03-01\n85.0\n183.0\n253.0\n\n\n2022-02-01\n126.0\n229.0\n92.0\n\n\n2022-01-01\n141.0\n282.0\n27.0",
    "crumbs": [
      "Personal",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environmental impact of home insulation</span>"
    ]
  },
  {
    "objectID": "Manuscripts/Environmental impact/index.html#emissions-of-energy-production",
    "href": "Manuscripts/Environmental impact/index.html#emissions-of-energy-production",
    "title": "Environmental impact of utilities",
    "section": "Emissions of energy production",
    "text": "Emissions of energy production\nTo quantify and compare the warming effects of different kind of emissions, the IPPC proposes using the Global Warming Potential (GWP), which can be used to express the warming effect of different emissions to that of CO₂. To calculate our total emissions, we must first determine the emissions caused by the energy production.\n\nElectricity\nThe emissions of electricity production depends on the source of the energy, which changes minute-by-minute. During day, a lot of green solar power is generated, and during peaks, gas turbines kick in. Exact information on the current national energy mix is publicly available. Ember-energy calculates the CO₂ emissions based on the energy mix, and has an API (email required) which provides the following numbers:\nSee also: https://www.cbs.nl/-/media/_excel/2023/06/1-co2-emissie-energieverbruik-rendementen-elektriciteit-2021.xls\n\n\n\n\n\n\n\n\nFigure 2.2: CO₂ emissions of the dutch energy production over time\n\n\n\n\n\nTo calculate the emissions caused by our energy consumption, we should account for the differing CO₂ emissions as follows:\n\\[\n\\text{CO}_2  = \\sum_{i=1}^{n} E_i \\times F_i\n\\] With \\(\\text{CO}_2\\) is the total produced CO₂ in grams,\n\\(E_i\\) the electricity usage for month \\(i\\) in kWh,\n\\(F_i\\) the emissions intensity in \\(g CO₂/kWh\\) for that specific month \\(i\\).\n\n\nGas\nCalculating the exact emissions caused by gas production is somewhat more complex as gas distributors measure the gas-usage as volume (m³) which is dependent on the temperature, pressure and gas mix, all of which are subject to change. Gas distributors solve this by multiply the measured volume with a correction value to determine the caloric value of the consumed gas (also see wobbe index). These corrections can be found on the final invoice.\nThe Netherlands Enterprise Agency (RVO) has calculated the emission factor for natural gas to be 56.34 kg CO₂ per GJ of energy. This only includes the emissions caused by burning the gas, not from producing it. The exact number differs by ±2% per year due to differences in the national gas mix, for instance through higher LNG imports.\nThe CBS reports that 1 GJ of natural gas corresponds to 31.6 m³, thus we can calculate the emissions per m³ as follows:\n\\[\n\\frac{56.34 \\text{ kg}}{\\text{GJ}}\n\\]\nSince\n\\[\n1 \\text{ GJ} = 31.6 \\text{ m}^3\n\\] we can compute:\n\\[\n\\frac{56.34}{31.6} \\text{ kg/m}^3\n\\]\nwhich simplifies to:\n\\[\n1.78 \\text{ kg CO}_2 \\text{ per m}^3\n\\]\nAs the deviations for the emissions of the gas mix are ~2%, we simplify the calculation by not accounting for them.",
    "crumbs": [
      "Personal",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environmental impact of home insulation</span>"
    ]
  },
  {
    "objectID": "Manuscripts/Environmental impact/index.html#calculations",
    "href": "Manuscripts/Environmental impact/index.html#calculations",
    "title": "Environmental impact of utilities",
    "section": "Calculations",
    "text": "Calculations\nFrom the emission factors per energy type the final formula can be determined:\n\\[\n\\text{CO}_2 = \\left( \\sum_{i=1}^{n} E_i \\times F_i \\right) + \\left( G \\times 1,78 \\right)\n\\] With \\(\\text{CO}_2\\) as the total produced CO₂ in grams,\n\\(E_i\\) the electricity usage for month \\(i\\) in kWh,\n\\(F_i\\) the emissions intensity in \\(kg CO₂/kWh\\) for that specific month \\(i\\). \\(G\\) the total gas usage in m³\nPlugging our usage data into this formula gives us the following emissions:\n\n\n\n\n\n\n\n\nFigure 2.3: Monthly CO₂ Emissions from Energy and Gas\n\n\n\n\n\n\n\n\n\n\nyear\nGas emissions (kg CO₂)\nElectricity emissions (kg CO₂)\nTotal emissions (kg CO₂)\n\n\n\n\n2022\n1279.82\n721.12\n2000.94\n\n\n2023\n1361.70\n551.89\n1913.59\n\n\n2024\n970.99\n452.84\n1423.83\n\n\n\n\n\n\n\n\n\nDetails\n\n\n\n\n\n\ndate\nElectricity\nGas\ntotal\nyear\n\n\n\n\n2024-11-01\n68.62542\n154.682\n223.30742\n2024\n\n\n2024-10-01\n48.07719\n60.164\n108.24119\n2024\n\n\n2024-09-01\n30.03240\n14.596\n44.62840\n2024\n\n\n2024-08-01\n23.54670\n14.418\n37.96470\n2024\n\n\n2024-07-01\n18.37869\n16.376\n34.75469\n2024\n\n\n2024-06-01\n23.69920\n17.266\n40.96520\n2024\n\n\n2024-05-01\n31.56969\n14.062\n45.63169\n2024\n\n\n2024-04-01\n29.12778\n76.184\n105.31178\n2024\n\n\n2024-03-01\n40.34709\n108.402\n148.74909\n2024\n\n\n2024-02-01\n63.42028\n206.480\n269.90028\n2024\n\n\n2024-01-01\n76.01712\n288.360\n364.37712\n2024\n\n\n2023-12-01\n65.73720\n256.320\n322.05720\n2023\n\n\n2023-11-01\n51.36404\n72.980\n124.34404\n2023\n\n\n2023-10-01\n34.19300\n24.920\n59.11300\n2023\n\n\n2023-09-01\n20.16000\n12.460\n32.62000\n2023\n\n\n2023-08-01\n14.96307\n17.800\n32.76307\n2023\n\n\n2023-07-01\n26.62884\n30.260\n56.88884\n2023\n\n\n2023-06-01\n22.32100\n35.600\n57.92100\n2023\n\n\n2023-05-01\n27.99544\n58.740\n86.73544\n2023\n\n\n2023-04-01\n48.38694\n137.060\n185.44694\n2023\n\n\n2023-03-01\n68.77440\n220.720\n289.49440\n2023\n\n\n2023-02-01\n85.97480\n206.480\n292.45480\n2023\n\n\n2023-01-01\n85.39542\n288.360\n373.75542\n2023\n\n\n2022-12-01\n89.38264\n231.400\n320.78264\n2022\n\n\n2022-11-01\n58.34738\n72.980\n131.32738\n2022\n\n\n2022-10-01\n67.56115\n42.720\n110.28115\n2022\n\n\n2022-09-01\n60.91374\n35.600\n96.51374\n2022\n\n\n2022-08-01\n48.43504\n21.360\n69.79504\n2022\n\n\n2022-07-01\n37.03166\n30.260\n67.29166\n2022\n\n\n2022-06-01\n42.82070\n42.720\n85.54070\n2022\n\n\n2022-05-01\n32.15056\n60.520\n92.67056\n2022\n\n\n2022-04-01\n42.86040\n115.700\n158.56040\n2022\n\n\n2022-03-01\n66.65775\n151.300\n217.95775\n2022\n\n\n2022-02-01\n66.27718\n224.280\n290.55718\n2022\n\n\n2022-01-01\n108.68562\n250.980\n359.66562\n2022\n\n\n\n\n\n\n\n\nIf real estate occupancy rate, area or any other measure is known, calculating the carbon footprint per person, workspace or area is trivial.",
    "crumbs": [
      "Personal",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environmental impact of home insulation</span>"
    ]
  },
  {
    "objectID": "Manuscripts/Environmental impact/index.html#concerns",
    "href": "Manuscripts/Environmental impact/index.html#concerns",
    "title": "Environmental impact of utilities",
    "section": "Concerns",
    "text": "Concerns\nClearly, the current calculations only accounts for the direct emissions caused by burning gas, or from producing electricity. (scope 1&2 emissions). Producing and transporting gas and electricity also has a significant impact on the environment.\nFurthermore, solar panels return electricity into the grid. How should they be accounted for?",
    "crumbs": [
      "Personal",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environmental impact of home insulation</span>"
    ]
  },
  {
    "objectID": "SCDA.html",
    "href": "SCDA.html",
    "title": "Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n\nCode\n1 + 1\n\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "University assignments",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html",
    "href": "projects/SCDA.html",
    "title": "Supply Chain Data Analytics",
    "section": "",
    "text": "3.1 Data selection\nWe analyze, forecast and interpret the Superstore sales provided by Tableau using different statistical and machine learning methods.\nThe dataset provided contains information about products, sales and profits of a fictitious US company. The dataset contains about 10,000 rows with 1,850 unique product names and 17 product subcategories, covering four consecutive years on a daily basis.\nWe describe our work in the PDF version. However, we would like to recommend reading our quarto manuscript here as it contains the relevant R code in the Article Notebook.",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html#data-pre-processing",
    "href": "projects/SCDA.html#data-pre-processing",
    "title": "Supply Chain Data Analytics",
    "section": "3.2 Data Pre-processing",
    "text": "3.2 Data Pre-processing\nThe superstore data set we selected is of high quality: At first glance (which needs to be verified during the visualization), the data appears to have been recorded regularly and without interruptions. There is no sign of a sudden structural change. Since the data are consumer products, it should contain both trends and seasonality. Nevertheless, we have included hypothetical steps to demonstrate our understanding of the data preprocessing procedure. In detail, we did:\n\n\nCode\n# Clear workspace\nrm(list = ls())\n# Function to load (and install if necessary) dependencies\ninstall_and_load &lt;- function(packages) {\n  install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)\n  invisible(lapply(packages, require, character.only = TRUE))\n}\ninstall_and_load(c(\"tidyverse\", \"readxl\", \"ggplot2\", \"lubridate\", \"stats\", \"Amelia\",\"forecast\", \"tseries\", \"plotly\", \"stringr\", \"knitr\", \"kableExtra\"))\n\n\n\nImproved column names by removing whitespaces\nRemoved the Row_ID column as it can be inferred by it’s index\nRemoved all columns with a single unique value, as storing these would be redundant\nEnsured machine-readable date formats in yyyy-mm-dd as these usually differ per locale.\nEnsured proper decimal separators\nCalculated the number of missing values (both NA and empty string ““) per column.\n\n\n\nCode\n# Load the data\nsuppressWarnings({data &lt;- read_excel(\"data/sample_-_superstore.xls\")}) # The Postal code column is stored as 'text' but coerced to numeric, causing warnings which we suppress\n\n# Improve column names\ncolnames(data) &lt;- str_replace_all(colnames(data), \" \", \"_\")\ncolnames(data) &lt;- str_replace_all(colnames(data), \"-\", \"_\")\n\n# Remove the 'Row_ID' column as it can be inferred by it's index\ndata &lt;- subset(data, select = -`Row_ID`)\n\n# Remove all columns that have only one unique value, as storing these would be redundant\ndata &lt;- data[, sapply(data, function(col) length(unique(col)) &gt; 1)]\n\n# Ensure a machine-readable date format as these are usually horrible in excel files\ndata$Order_Date &lt;- as.Date(data$Order_Date, format = \"%Y-%m-%d\")\ndata$Ship_Date &lt;- as.Date(data$Ship_Date, format = \"%Y-%m-%d\")\n\n# The readxl package by default uses the correct decimal separator (as opposed to base R)\n\n# Calculate the number of missing values per column.\n# Origional dates and R date objects are in unix time, which return NA when compared to text (empty string). These dates are stored as 'double' datatype, Thus we check character columns for empty strings, and all columns for NA values. \nmissing_values &lt;- sapply(data, function(col) {\n  if (inherits(col, \"Date\")) {\n    sum(is.na(col))\n  } else if (is.character(col)) {\n    sum(is.na(col) | col == \"\")\n  } else {\n    sum(is.na(col))\n  }\n})\n\n# sum(missing_values) returns 0!\n\n# Optionally, print the missing values as a nice table\nmissing_values_table &lt;- data.frame(\n  Column = names(missing_values),\n  Missing_or_Empty = missing_values\n)\n# Note that there are no missing values, thus we do not print them\n# kable(missing_values_table, caption = \"Missing or Empty Values in Columns\", format = \"pipe\")\n\n\nrm(missing_values, missing_values_table)\n\n\nAfter these steps (and transposing the table for better document formatting), the data looks as follows:\n\n\nCode\nkable(t(head(data, 3)), caption = \"First 3 Rows of the Data (Transposed)\", format = \"markdown\")\n\n\n\nFirst 3 Rows of the Data (Transposed)\n\n\n\n\n\n\n\n\nOrder_ID\nCA-2016-152156\nCA-2016-152156\nCA-2016-138688\n\n\nOrder_Date\n2016-11-08\n2016-11-08\n2016-06-12\n\n\nShip_Date\n2016-11-11\n2016-11-11\n2016-06-16\n\n\nShip_Mode\nSecond Class\nSecond Class\nSecond Class\n\n\nCustomer_ID\nCG-12520\nCG-12520\nDV-13045\n\n\nCustomer_Name\nClaire Gute\nClaire Gute\nDarrin Van Huff\n\n\nSegment\nConsumer\nConsumer\nCorporate\n\n\nCity\nHenderson\nHenderson\nLos Angeles\n\n\nState\nKentucky\nKentucky\nCalifornia\n\n\nPostal_Code\n42420\n42420\n90036\n\n\nRegion\nSouth\nSouth\nWest\n\n\nProduct_ID\nFUR-BO-10001798\nFUR-CH-10000454\nOFF-LA-10000240\n\n\nCategory\nFurniture\nFurniture\nOffice Supplies\n\n\nSub_Category\nBookcases\nChairs\nLabels\n\n\nProduct_Name\nBush Somerset Collection Bookcase\nHon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back\nSelf-Adhesive Address Labels for Typewriters by Universal\n\n\nSales\n261.96\n731.94\n14.62\n\n\nQuantity\n2\n3\n2\n\n\nDiscount\n0\n0\n0\n\n\nProfit\n41.9136\n219.5820\n6.8714\n\n\n\n\n\nWe did not find any missing values, confirming the quality of the data set. There is some more processing to do, for instance the removal of outliers. However, by doing so we impose our own assumptions on the data. Let’s start by evaluating the descriptive statistics of our data and check if further processing is required.\n\n\nCode\ndescriptive_statistics &lt;- function(column) {\n  if (is.numeric(column)) {\n    stats &lt;- list(\n      Min = min(column, na.rm = TRUE), # Note that handling NA values increases robustness (and I copied the funciton from some of my earlier work)\n      Max = max(column, na.rm = TRUE),\n      Mean = mean(column, na.rm = TRUE),\n      Median = median(column, na.rm = TRUE),\n      StdDev = sd(column, na.rm = TRUE)\n    )\n  } else if (inherits(column, \"Date\")) {\n    stats &lt;- list(\n      Earliest = format(min(column, na.rm = TRUE), \"%Y-%m-%d\"),\n      Latest = format(max(column, na.rm = TRUE), \"%Y-%m-%d\")\n    )\n  } else if (is.character(column)) {\n    stats &lt;- list(\n      Unique = length(unique(column)),\n      Mode = names(sort(table(column), decreasing = TRUE)[1])\n    )\n  } else {\n    stats &lt;- NULL\n  }\n  return(stats)\n}\n\n# Call function on dataframe\ndescriptive_stats &lt;- lapply(data, descriptive_statistics)\n\n# Separate to tables dependent on data type\nnumeric_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.numeric(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ndate_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (inherits(data[[col_name]], \"Date\")) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ncharacter_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.character(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\n\n\n\n\nCode\nkable(\n  numeric_stats,\n  caption = \"Descriptive Statistics for Numeric Columns\",\n  format = \"pipe\")\n\n\n\nDescriptive Statistics for Numeric Columns\n\n\nColumn\nMin\nMax\nMean\nMedian\nStdDev\n\n\n\n\nPostal_Code\n1040\n99301\n55190.38\n56430.5\n32063.69\n\n\nSales\n0.444\n22638.48\n229.858\n54.49\n623.2451\n\n\nQuantity\n1\n14\n3.789574\n3\n2.22511\n\n\nDiscount\n0\n0.8\n0.1562027\n0.2\n0.206452\n\n\nProfit\n-6599.978\n8399.976\n28.6569\n8.6665\n234.2601\n\n\n\n\n\nCode\nkable(\n  date_stats,\n  caption = \"Descriptive Statistics for Date Columns\",\n  format = \"pipe\")\n\n\n\nDescriptive Statistics for Date Columns\n\n\nColumn\nEarliest\nLatest\n\n\n\n\nOrder_Date\n2014-01-03\n2017-12-30\n\n\nShip_Date\n2014-01-07\n2018-01-05\n\n\n\n\n\nWe inspect the orders with the lowest and highest Sales amount (in USD). The most expensive orders were professional printers, cameras and teleconferencing units with high unit prices. The orders with the lowest sales amount were often binders and had a high Discount rate.\nInterestingly there are orders with a negative profit. They typically have high Discount rates and often concern the same item, such as the “Cubify CubeX 3D Printer Triple Head Print”. The orders with a negative Profit were often part of a larger order (for instance CA-2016-108196), and placed by customers with multiple orders. We suspect these negative Profit’s to be caused by items of lower quality that receive discounts, general discount codes, or volume discounts. However, due to the high discounts especially on orders with negative profit, we assume these to be valid orders.\n** Some negative profit products **\nIn figure x we plotted the quantities of the most sold products. Unfortunately, the sold quantities of individual products were too low to determine any meaningful trends.\n\n\nCode\n# Optionally: print top 10 sale quantity barplot\n# # Sum of Quantity for top products\n# top_products &lt;- data %&gt;%\n#   group_by(Product_Name) %&gt;%\n#   summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%\n#   arrange(desc(total_quantity)) %&gt;%\n#   slice_head(n = 10) %&gt;% \n#   mutate(ProdName8 = substr(Product_Name, 1, 8)) # Truncate product names to the first 8 characters. Long names mess up formatting\n# \n# # Plot\n# ggplot(top_products, aes(x = reorder(ProdName8, -total_quantity), y = total_quantity)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product ID\",\n#        y = \"Total Quantity\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Aggregate quantity by Product Name and Order Date to create a time series\ntime_series_data &lt;- data %&gt;%\n  group_by(Product_Name, Order_Date) %&gt;%\n  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%\n  ungroup()\n# Filter for the top products by total quantity sold (adjust as needed)\ntop_products &lt;- time_series_data %&gt;%\n  group_by(Product_Name) %&gt;%\n  summarize(total_quantity = sum(total_quantity)) %&gt;%\n  arrange(desc(total_quantity)) %&gt;%\n  slice_head(n = 10)  # Select top 10 products\n\n# Filter the time-series data for only these top products\nfiltered_time_series_data &lt;- time_series_data %&gt;%\n  filter(Product_Name %in% top_products$Product_Name) %&gt;%\n  mutate(ProdName10 = substr(Product_Name, 1, 10)) # Product names can be quite long and mess up layouts. Lets only plot the first 10 chars.\n\n# Here we do some special plotting. We want to show the plot with only one selected line by default, but make sure that the other 9 top sold products can be selected. We first create the ggplotly object, and than modify the visibility of the traces\n\n\n\n\nCode\n# Plot interactive figure in html, plot ggplot2 in pdf:\n\n# Creating the ggplotly object\np_ly &lt;- ggplotly(ggplot(filtered_time_series_data, aes(x = Order_Date, y = total_quantity, color = ProdName10)) +\n  geom_line(size = 1) +\n  labs(title = \"Quantity Sold Over Time per Product\",\n       x = \"Order Date\",\n       y = \"Quantity Sold\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_discrete(name = \"Product Name\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible &lt;- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n\n\n\n\nFigure X Sale quantity of the most popular products\n\n\nOur proposed workaround is to aggregate Product_Name by Sub_Category, and treat it as a single product for the rest of the assignment, which we plotted in figure X.\n\n\nCode\n# Bar plots\n\n# # Count frequency of top 20 products\n# top_products &lt;- data %&gt;%\n#   count(Product_Name, sort = TRUE) %&gt;%\n#   top_n(20, n) %&gt;%\n#   mutate(ProdName8 = substr(Product_Name, 1, 8))\n# \n# # Plot!\n# ggplot(top_products, aes(x = reorder(`ProdName8`, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n# \n# Count frequency of top 20 products\ntop_categories &lt;- data %&gt;%\n  count(Sub_Category, sort = TRUE)\n# \n# # Plot!\n# ggplot(top_categories, aes(x = reorder(Sub_Category, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Sub_Categories sorted\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Find top 10 most sold product names\ntop_10_categories &lt;- top_categories$Sub_Category[0:10]\n\n# Filter the data for  top 10 products\ntop_10_data &lt;- data %&gt;% filter(Sub_Category %in% top_10_categories)\n\n# calculate sales per month\ntop_10_data &lt;- top_10_data %&gt;%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each sub-category\ntop_10_data_aggregated &lt;- top_10_data %&gt;%\n  group_by(Month, Sub_Category) %&gt;%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Some special interactive plot formatting (see previous plot)\np_ly &lt;- ggplotly(ggplot(top_10_data_aggregated, aes(x = Month, y = Sales_Count, color = Sub_Category, group = Sub_Category)) +\n    geom_line(size = 1) +\n    geom_point(size = 2) +\n    labs(title = \"Monthly Sales for the Top 3 Most Sold Products\",\n         x = \"Month\",\n         y = \"Sales Count\",\n         color = \"Product Name\") +\n    theme_minimal())\n\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible &lt;- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n\n\n\n\n\n\nThis aggregated Quantity starts to show trends and seasonality, and is much more useful to base predictions on! We will use these aggregated sub-categories for the rest of the assignment.\nTo properly finish our data preprocessing we ran some statistics on Quantity aggregated by Sub_Category. Table x contains some descriptive statistics.\n\n\nCode\nlibrary(dplyr)\nlibrary(kableExtra)\n\n# Summarize the data\noutlier_summary &lt;- data %&gt;%\n  group_by(Sub_Category) %&gt;%\n  summarize(\n    Min = round(min(Quantity), 2),\n    Mean = round(mean(Quantity), 2),\n    Max = round(max(Quantity), 2),\n    Sd = round(sd(Quantity), 2),\n    CI_lower = round(Mean - 1.96 * (Sd / sqrt(n())), 2),\n    CI_upper = round(Mean + 1.96 * (Sd / sqrt(n())), 2),\n    .groups = \"drop\"\n  )\n\n# Output tables\nkable(\n  outlier_summary,\n  caption = \"Statistics for Sub_Category quantity\",\n  format = \"pipe\")\n\n\n\nStatistics for Sub_Category quantity\n\n\nSub_Category\nMin\nMean\nMax\nSd\nCI_lower\nCI_upper\n\n\n\n\nAccessories\n1\n3.84\n14\n2.28\n3.68\n4.00\n\n\nAppliances\n1\n3.71\n14\n2.12\n3.52\n3.90\n\n\nArt\n1\n3.77\n14\n2.13\n3.62\n3.92\n\n\nBinders\n1\n3.92\n14\n2.29\n3.80\n4.04\n\n\nBookcases\n1\n3.81\n13\n2.28\n3.51\n4.11\n\n\nChairs\n1\n3.82\n14\n2.28\n3.64\n4.00\n\n\nCopiers\n1\n3.44\n9\n1.83\n3.01\n3.87\n\n\nEnvelopes\n1\n3.57\n9\n2.05\n3.32\n3.82\n\n\nFasteners\n1\n4.21\n14\n2.41\n3.89\n4.53\n\n\nFurnishings\n1\n3.72\n14\n2.16\n3.58\n3.86\n\n\nLabels\n1\n3.85\n14\n2.35\n3.61\n4.09\n\n\nMachines\n1\n3.83\n11\n2.17\n3.43\n4.23\n\n\nPaper\n1\n3.78\n14\n2.23\n3.66\n3.90\n\n\nPhones\n1\n3.70\n14\n2.19\n3.56\n3.84\n\n\nStorage\n1\n3.73\n14\n2.19\n3.58\n3.88\n\n\nSupplies\n1\n3.41\n10\n1.84\n3.15\n3.67\n\n\nTables\n1\n3.89\n13\n2.45\n3.62\n4.16\n\n\n\n\n\nThe statistics for Quantity aggregated by Sub_Category looks valid. We can visualize it as histogram and check for anomalies. Figure y contains histograms of Quantity per Sub_Category.\n\n\nCode\nsub_categories &lt;- unique(data$Sub_Category)\n\np &lt;- plot_ly()\nfor (i in seq_along(sub_categories)) {\n  sub &lt;- sub_categories[i]\n  subset_data &lt;- data %&gt;% filter(Sub_Category == sub)\n  p &lt;- add_trace(\n    p,\n    x = subset_data$Quantity,\n    type = \"histogram\",\n    name = sub,\n    visible = ifelse(i == 1, TRUE, FALSE)\n  )\n}\n\n# We add a drop down menu for Sub_Category as toggling visibility in default ggplot2 adds the histograms up. Instead we want to be able to show each histogram seperately. \ndropdown_buttons &lt;- lapply(seq_along(sub_categories), function(i) {\n  list(\n    method = \"update\",\n    args = list(\n      list(visible = lapply(seq_along(sub_categories), function(j) j == i)),\n      list(xaxis = list(title = \"Quantity\", autorange = TRUE), \n           yaxis = list(title = \"Frequency\", autorange = TRUE))\n    ),\n    label = sub_categories[i]\n  )\n})\n\n# Style drop down layout\np &lt;- p %&gt;%\n  layout(\n    title = \"Distribution of Quantity Sold per Order by Sub-Category\",\n    xaxis = list(title = \"Quantity\"),\n    yaxis = list(title = \"Frequency\"),\n    showlegend = FALSE,  # Drop down instead of legend\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        buttons = dropdown_buttons,\n        direction = \"down\",\n        x = 0.99,\n        y = 0.99,\n        showactive = TRUE,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    )\n  )\np\n\n\n\n\n\n\nThe histograms show that the quantities a right-skewed distributed. This is to be expected since most orders contain only a small number of items. We will not remove the outliers with large quantities since they appear valid..",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html#data-visualization",
    "href": "projects/SCDA.html#data-visualization",
    "title": "Supply Chain Data Analytics",
    "section": "3.3 Data Visualization",
    "text": "3.3 Data Visualization",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html#forecasting-method-evaluation",
    "href": "projects/SCDA.html#forecasting-method-evaluation",
    "title": "Supply Chain Data Analytics",
    "section": "3.4 Forecasting Method Evaluation",
    "text": "3.4 Forecasting Method Evaluation\n\n3.4.1 Forecasting top 3 product categories (4a)\nLet’s forecast sold quantities for the three most sold sub-categories:\nThe steps taken for data preparation were:\n\nIdentifying Top Subcategories: The top three subcategories are selected from our dataset based on their sold quantities. The top three were: Binders, furnishing and paper.\nThe sold quantities are aggregated monthly to create a time series object which we can use in the forecasting.\nA KPSS showed that the data is non stationary. First-order differencing is applied to transform the data from non-stationary to stationary. The KPSS results in a p-value &gt;0.05 showing the stationarity.\n\n\n\nCode\n# Find top 3 most sold product names\ntop_categories &lt;- data %&gt;%\n  group_by(Sub_Category) %&gt;%\n  summarise(Total_Quantity = sum(Quantity)) %&gt;%\n  arrange(desc(Total_Quantity))\ntop_3_subcategories &lt;- top_categories$Sub_Category[0:3]\n\n# Filter the data for  top 3 products\ntop_3_data &lt;- data %&gt;% filter(Sub_Category %in% top_3_subcategories)\n\n# calculate sales per month\ntop_3_data &lt;- top_3_data %&gt;%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each product\ntop_3_data_aggregated &lt;- top_3_data %&gt;%\n  group_by(Month, Sub_Category) %&gt;%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Create a time series object for each product\nts_data &lt;- top_3_data_aggregated %&gt;%\n  pivot_wider(names_from = Sub_Category, values_from = Sales_Count, values_fill = 0) %&gt;%\n  select(-Month) %&gt;%\n  as.matrix()\n\n# Create a time series object\nts_data &lt;- ts(ts_data, start = c(2014, 1), end = c(2017, 12), frequency = 12)\n\n# Create a time series list for each subcategory\nts_list &lt;- list()\n\nfor (subcategory in top_3_subcategories) {\n  # Filter data for the subcategory\n  subcategory_data &lt;- top_3_data_aggregated %&gt;% filter(Sub_Category == subcategory)\n\n  # Create a time series object (assuming monthly data from January 2014 to December 2017)\n  ts_list[[subcategory]] &lt;- ts(subcategory_data$Sales_Count,\n                                start = c(2014, 1),\n                                end = c(2017, 12),\n                                frequency = 12)\n}\n\n#### 4 A\n# Step 4: Apply forecasting methods to the top 3 sub-categories\nforecast_results &lt;- list()  # Store results\n\nfor (subcategory in names(ts_list)) {\n  ts_current &lt;- ts_list[[subcategory]]\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size &lt;- floor(0.7 * length(ts_current))\n  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model &lt;- auto.arima(train_ts)\n  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model &lt;- HoltWinters(train_ts)\n  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model &lt;- ets(train_ts)\n  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] &lt;- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n\n# For formatting, we ommitted almost all output. You can uncomment the code and check your output if you like.\n\n# # Step 5: Print results\n#  for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n\n\nThree models are applied to each subcategory to forecast it. The models we use are: ARIMA, Holt-Winters and ETS. We have chosen these models because of their level of suitability for discrete time series data with all different levels of trend and seasonality. To evaluate the methods and its effectiveness , the data is split into a training set (70%) and testing set (30%).\nTo assess the results, we use the following performance metrics: ME, RMSE, MAE and MAPE. They are calculated for the training and testing phases of the forecast.\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE if you compare it to the other models.\n\n\nCode\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current &lt;- ts_list[[subcategory]]\n  train_size &lt;- floor(0.7 * length(ts_current))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions(warn = -1)\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast, main = paste(\"ARIMA Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast, main = paste(\"Holt-Winters Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast, main = paste(\"ETS Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n#more stationary tests\n# Perform KPSS Test for the top 3 subcategories\n# top_3_subcategories &lt;- top_categories$Sub_Category[0:3]\n# \n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(ts_list)) {\n#     ts_current &lt;- ts_list[[subcategory]]\n#     cat(\"\\nKPSS Test for Sub-Category:\", subcategory, \"\\n\")\n#     print(kpss.test(ts_current))\n#   } else {\n#     cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n#   }\n# }\n\n#Because the all the 3 subcategory are non stationary because of a P value which is &lt;=0.05 we need to use differencing\n# Apply differencing to each of the top 3 subcategories\ndifferenced_series &lt;- list()\n\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(ts_list)) {\n    ts_current &lt;- ts_list[[subcategory]]  # Get the time series for the subcategory\n    ts_diff &lt;- diff(ts_current, differences = 1)  # Apply first-order differencing\n    differenced_series[[subcategory]] &lt;- ts_diff  # Store the differenced series\n\n    # Recheck stationarity with KPSS test\n    #cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_diff))\n  } else {\n    #cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n  }\n}\n\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n\n\n\n\nCode\n# Time differenced plots\n# # Now P value is larger then 0.05 so we have stationary data\n# # Plot the differenced series for each subcategory\n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(differenced_series)) {\n#     ts_diff &lt;- differenced_series[[subcategory]]\n#     cat(\"\\nPlotting Differenced Series for Sub-Category:\", subcategory, \"\\n\")\n#     plot(ts_diff, main = paste(\"Differenced Series for Sub-Category:\", subcategory),\n#          ylab = \"Differenced Values\", xlab = \"Time\")\n#   }\n# }\n\n\n# Combine differenced series plots for all top subcategories\ncombined_diff_plot &lt;- function(differenced_series, top_3_subcategories) {\n  plot(NULL, xlim = range(time(differenced_series[[top_3_subcategories[1]]])), \n       ylim = range(sapply(differenced_series[top_3_subcategories], range, na.rm = TRUE)),\n       xlab = \"Time\", ylab = \"Differenced Values\",\n       main = \"Differenced Series for Top Subcategories\")\n  colors &lt;- c(\"blue\", \"green\", \"purple\")\n  for (i in seq_along(top_3_subcategories)) {\n    subcategory &lt;- top_3_subcategories[i]\n    if (subcategory %in% names(differenced_series)) {\n      ts_diff &lt;- differenced_series[[subcategory]]\n      lines(ts_diff, col = colors[i], lwd = 2, lty = i)\n    }\n  }\n  legend(\"topright\", legend = top_3_subcategories, \n         col = colors, lty = 1:length(top_3_subcategories), lwd = 2)\n}\ncombined_diff_plot(differenced_series, top_3_subcategories)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#NEW FORECASTING FOR 4A with stationary data\n# Step 4: Apply forecasting methods to the differenced top 3 sub-categories\nforecast_results &lt;- list()  # Store results\n\nfor (subcategory in names(differenced_series)) {\n  ts_current &lt;- differenced_series[[subcategory]]  # Use the differenced series\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size &lt;- floor(0.7 * length(ts_current))\n  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model &lt;- auto.arima(train_ts)\n  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model &lt;- HoltWinters(train_ts)\n  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model &lt;- ets(train_ts)\n  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] &lt;- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n# # Step 5: Print results\n# for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast,\n#        main = paste(\"ARIMA Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast,\n#        main = paste(\"Holt-Winters Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast,\n#        main = paste(\"ETS Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current &lt;- ts_list[[subcategory]]\n  train_size &lt;- floor(0.7 * length(ts_current))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# final KPSS test\n\n# Perform KPSS Test for the differenced series in the top 3 subcategories\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(differenced_series)) {\n    ts_current &lt;- differenced_series[[subcategory]]  # Get the differenced series\n    cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_current))\n  } else {\n    cat(\"\\nSub-Category not found in differenced_series:\", subcategory, \"\\n\")\n  }\n}\n\n\n\nKPSS Test for Differenced Sub-Category: Binders \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Paper \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Furnishings \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n\n\nCode\n# now they are all 0.1\n\n\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE. - ARIMA Binders: - Forecasting results Binders:\nFor the subcategory furnishings we can see that the ETS forecasting method is the most stable across the training and testing phase. - ETS furnishings - Forecasting results Furnishings:\nFor the last subcategory and product paper the ETS model is again the most consistent, comparing the statistics for training and test set. The high variability in the test data leads to larger forecasting errors in all the 3 models.\n\nETS Furnishings:\nForecasting results:\n\nResidual Diagnostics: - The checks show no real autocorrelation for ARIMA models. Which indicates a good fitting forecast.\n\n\n3.4.2 Conclusion (4a)\nThe most effective model is not the same in all the subcategories. Each model was validated based on its ability to capture seasonality and trend. ARIMA performed better for Binders, while ETS performed better for Furnishings and Paper.\n\n\n3.4.3 Clustering (4b)\n\n\nCode\n# 4B\n# 4B: Group Products into Clusters and Apply Forecasting Techniques\n# Step 1: Extract Time-Series Features for Clustering\ntime_series_features &lt;- data.frame()\n\nfor (subcategory in names(ts_list)) {\n  ts_current &lt;- ts_list[[subcategory]]\n\n  # Decompose the time series to extract features\n  decomposition &lt;- decompose(ts_current)\n  trend_strength &lt;- var(decomposition$trend, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  seasonal_strength &lt;- var(decomposition$seasonal, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  random_strength &lt;- var(decomposition$random, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n\n  # Store extracted features\n  time_series_features &lt;- rbind(time_series_features,\n                                data.frame(SubCategory = subcategory,\n                                           TrendStrength = trend_strength,\n                                           SeasonalStrength = seasonal_strength,\n                                           RandomStrength = random_strength))\n}\n\n# Step 2: Normalize the Features for Clustering\ntime_series_features_scaled &lt;- time_series_features %&gt;%\n  select(-SubCategory) %&gt;%\n  scale()\n\n# verify rows\n# nrow(time_series_features_scaled)\n\n# Step 3: Perform K-Means Clustering\n# Determine the number of clusters dynamically\nk &lt;- min(3, nrow(time_series_features_scaled))  # Set k to the smaller of 3 or the number of rows\n# Hierarchical Clustering\ndistance_matrix &lt;- dist(time_series_features_scaled)  # Calculate distance matrix\nhc &lt;- hclust(distance_matrix)  # Perform hierarchical clustering\ntime_series_features$Cluster &lt;- cutree(hc, k = k)  # Cut tree into 'k' clusters\n# Add cluster information to the original data\ntime_series_features$Cluster &lt;- cutree(hc, k = k)\n\n# Step 4: Apply Forecasting Techniques to Each Cluster\nforecast_results_by_cluster &lt;- list()\n\nfor (cluster_id in unique(time_series_features$Cluster)) {\n  # Get subcategories in the current cluster\n  subcategories_in_cluster &lt;- time_series_features$SubCategory[time_series_features$Cluster == cluster_id]\n\n  # Initialize storage for cluster results\n  cluster_forecast_results &lt;- list()\n\n  for (subcategory in subcategories_in_cluster) {\n    if (subcategory %in% names(ts_list)) {\n      ts_current &lt;- ts_list[[subcategory]]  # Access the time series\n\n      # Split the data into training and validation sets (70% training, 30% testing)\n      train_size &lt;- floor(0.7 * length(ts_current))\n      train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n      test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n      # 1. ARIMA\n      arima_model &lt;- auto.arima(train_ts)\n      arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n      arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n      # 2. Holt-Winters\n      hw_model &lt;- HoltWinters(train_ts)\n      hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n      hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n      # 3. ETS\n      ets_model &lt;- ets(train_ts)\n      ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n      ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n      # Store results for the subcategory\n      cluster_forecast_results[[subcategory]] &lt;- list(\n        ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n        HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n        ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n      )\n    } else {\n      cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n    }\n  }\n\n  # Store results for the cluster\n  forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]] &lt;- cluster_forecast_results\n}\n\n# Step 5: Print Forecasting Accuracy for Each Cluster\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cat(\"\\n\\nResults for\", cluster_id, \"\\n\")\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nSub-Category:\", subcategory, \"\\n\")\n\n    cat(\"\\nARIMA Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ARIMA$Accuracy)\n\n    cat(\"\\nHolt-Winters Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$HoltWinters$Accuracy)\n\n    cat(\"\\nETS Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ETS$Accuracy)\n  }\n}\n\n\n\n\nResults for Cluster_1 \n\nSub-Category: Binders \n\nARIMA Accuracy:\n                    ME      RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.7706014  4.643476 2.982256  0.6865304 12.91204 0.4854835\nTest set     5.9407398 10.783528 7.616473 10.3681817 17.32927 1.2398909\n                   ACF1 Theil's U\nTraining set 0.04429472        NA\nTest set     0.04929320 0.3573866\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.8668058 5.215491 3.789990 -0.4040374 15.36545 0.6169751\nTest set     2.2473496 8.712049 6.243226  0.1597635 16.02219 1.0163391\n                     ACF1 Theil's U\nTraining set -0.389045966        NA\nTest set     -0.001830843 0.2777843\n\nETS Accuracy:\n                   ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.743354  4.712561 3.656409  1.358692 15.41708 0.5952293\nTest set     7.439784 12.216161 8.825094 10.667088 20.93561 1.4366433\n                   ACF1 Theil's U\nTraining set -0.2225537        NA\nTest set      0.0608554 0.3767484\n\n\nResults for Cluster_2 \n\nSub-Category: Paper \n\nARIMA Accuracy:\n                    ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set  1.117384  6.309464  3.827714   1.702165 14.32157 0.5468163\nTest set     -9.014128 12.230774 10.375521 -30.103886 31.89519 1.4822173\n                     ACF1 Theil's U\nTraining set -0.007064333        NA\nTest set      0.108516273 0.6984566\n\nHolt-Winters Accuracy:\n                     ME     RMSE       MAE        MPE     MAPE      MASE\nTraining set   1.509544  7.27986  4.854547   3.325281 16.41309 0.6935067\nTest set     -12.036865 14.48061 13.347292 -39.791842 41.51609 1.9067560\n                    ACF1 Theil's U\nTraining set -0.02710216        NA\nTest set      0.10006713  0.845232\n\nETS Accuracy:\n                     ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set 0.60941117  6.204628 4.474990 -1.66197953 18.73240 0.6392842\nTest set     0.08500424 11.304488 8.247017 -0.08462612 20.47598 1.1781453\n                    ACF1 Theil's U\nTraining set 0.005205508        NA\nTest set     0.341519997  0.582549\n\n\nResults for Cluster_3 \n\nSub-Category: Furnishings \n\nARIMA Accuracy:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.0050974 3.873555 2.567868 -6.624673 20.20958 0.5559302\nTest set     3.9523810 7.782677 6.238095 10.720079 22.75340 1.3505155\n                    ACF1 Theil's U\nTraining set -0.20160077        NA\nTest set     -0.03570035 0.6102339\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.9137655 4.164677 3.475419 7.490317 24.75583 0.7524103\nTest set     3.6371987 7.200985 5.673333 9.724318 20.50134 1.2282474\n                    ACF1 Theil's U\nTraining set -0.43317305        NA\nTest set      0.01804785 0.5689788\n\nETS Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set 0.7370579 3.466690 2.851745  0.4301648 20.85220 0.617388\nTest set     6.0973038 8.354163 6.690832 20.7276401 23.55317 1.448531\n                   ACF1 Theil's U\nTraining set -0.2087083        NA\nTest set      0.3729315 0.7455977\n\n\nCode\n## Step 6: Visualize the Clusters\nlibrary(ggplot2)\n\nggplot(time_series_features, aes(x = TrendStrength, y = SeasonalStrength, color = as.factor(Cluster))) +\n  geom_point(size = 3) +\n  labs(title = \"Clusters of Subcategories Based on Time-Series Features\",\n       x = \"Trend Strength\", y = \"Seasonal Strength\", color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n#\n#check\n#residual diagnostic\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nResidual Diagnostics for Sub-Category:\", subcategory, \"\\n\")\n    checkresiduals(cluster_results[[subcategory]]$ARIMA$Model)\n  }\n}\n\n\n\nResidual Diagnostics for Sub-Category: Binders \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)(0,1,0)[12]\nQ* = 2.6295, df = 5, p-value = 0.7569\n\nModel df: 2.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Paper \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(0,1,0)[12]\nQ* = 6.6676, df = 6, p-value = 0.3527\n\nModel df: 1.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Furnishings \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,0,0)(0,1,0)[12] with drift\nQ* = 9.5952, df = 7, p-value = 0.2127\n\nModel df: 0.   Total lags used: 7\n\n\nCode\n# P-value is higher then 0.1 so we have stationary data, this is good\n#cluster level metrics\ncluster_metrics &lt;- data.frame()\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n  cluster_rmse &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"RMSE\"])\n  cluster_mape &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"MAPE\"])\n  cluster_metrics &lt;- rbind(cluster_metrics, data.frame(Cluster = cluster_id, MeanRMSE = mean(cluster_rmse), MeanMAPE = mean(cluster_mape)))\n}\nprint(cluster_metrics)\n\n\n    Cluster  MeanRMSE MeanMAPE\n1 Cluster_1 10.783528 17.32927\n2 Cluster_2 12.230774 31.89519\n3 Cluster_3  7.782677 22.75340\n\n\nCluster 1 (e.g., Binders): ARIMA outperformed other methods due to significant autocorrelation and trend components.\nCluster 2 (e.g., Furnishings): ETS was the most accurate method, effectively balancing trend and seasonality.\nCluster 3 (e.g., Paper): ETS also performed best, with ARIMA showing higher error rates due to variability in random components.\nResidual diagnostics were performed for all ARIMA models, confirming no significant autocorrelation (p &gt; 0.05).\nCluster-Level Metrics based on mean RMSE and MAPE show: - Cluster 1 had the lowest RMSE using ARIMA. - Cluster 2 and 3 were better modeled with ETS\n\n\n3.4.4 Conclusion (4b)\nClustering allows for tailored forecasting strategies. We conclude that for the given data set ARIMA is more effective for clusters with strong trends, while ETS is preferable for clusters with mixed seasonal and trend characteristics. The approach aligns with lecture notes, emphasizing the importance of adapting models based on time series characteristics.",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html#forecasting-future-values",
    "href": "projects/SCDA.html#forecasting-future-values",
    "title": "Supply Chain Data Analytics",
    "section": "3.5 Forecasting future values",
    "text": "3.5 Forecasting future values\n\n3.5.1 Forecasting 3 products (5a)\nIn this session, we focused on evaluating different forecasting models (ARIMA, Holt-Winters, and ETS) for multiple sub-categories by analyzing their accuracy metrics, such as RMSE, MAPE, and residual diagnostics. Based on the evaluation results, we selected the best-performing model for each sub-category. We then used these models to forecast the future outcomes for each sub-category, projecting the data for the next year. Note: we may need to interpret the outcomes and explain why we pick the certain model\n\n\nCode\n#Binders-&gt;choose ARIMA\nbinders_ts &lt;- ts_list[[\"Binders\"]]\narima_model &lt;- auto.arima(binders_ts)\nsummary(arima_model)\n\n\nSeries: binders_ts \nARIMA(1,1,1)(0,1,0)[12] \n\nCoefficients:\n          ar1      ma1\n      -0.4781  -0.4819\ns.e.   0.2324   0.2426\n\nsigma^2 = 51.18:  log likelihood = -117.97\nAIC=241.94   AICc=242.72   BIC=246.61\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 0.864453 5.931761 4.092168 -1.363101 15.06142 0.558023 -0.03746012\n\n\nCode\narima_forecast &lt;- forecast(arima_model, h = 12)\nprint(arima_forecast)\n\n\n         Point Forecast    Lo 80     Hi 80     Lo 95     Hi 95\nJan 2018       32.48390 23.31571  41.65208 18.462370  46.50543\nFeb 2018       23.77181 14.59632  32.94731  9.739103  37.80452\nMar 2018       45.85265 35.59989  56.10541 30.172412  61.53289\nApr 2018       46.20475 35.63662  56.77287 30.042185  62.36730\nMay 2018       44.08009 32.93968  55.22051 27.042295  61.11789\nJun 2018       44.61784 33.06359  56.17210 26.947139  62.28855\nJul 2018       40.36072 28.34867  52.37277 21.989869  58.73157\nAug 2018       44.48366 32.05796  56.90937 25.480189  63.48714\nSep 2018       73.42488 60.58630  86.26346 53.789962  93.05979\nOct 2018       51.45299 38.22024  64.68573 31.215253  71.69072\nNov 2018       72.43955 58.82134  86.05775 51.612293  93.26680\nDec 2018       89.44597 75.45417 103.43777 68.047363 110.84458\n\n\nCode\nplot(arima_forecast, main = \"ARIMA Forecast for Binders (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\nCode\n#Paper-&gt;choose ETS\npaper_ts &lt;- ts_list[[\"Paper\"]]\nets_model &lt;- ets(paper_ts)\nsummary(ets_model)\n\n\nETS(M,N,A) \n\nCall:\nets(y = paper_ts)\n\n  Smoothing parameters:\n    alpha = 0.3075 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 22.5954 \n    s = 17.4472 16.5763 -4.1253 15.2986 0.421 -5.102\n           -0.6145 -0.0341 -7.985 -2.6766 -15.6576 -13.5481\n\n  sigma:  0.2365\n\n     AIC     AICc      BIC \n365.1517 380.1517 393.2197 \n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1\nTraining set 1.450303 6.386648 4.166875 1.75373 14.03399 0.5245018 0.03600045\n\n\nCode\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2018       30.45588 21.22661 39.68516 16.34092 44.57085\nFeb 2018       28.34651 19.27484 37.41819 14.47258 42.22044\nMar 2018       41.32776 28.18367 54.47185 21.22561 61.42990\nApr 2018       36.01933 23.73891 48.29976 17.23804 54.80062\nMay 2018       43.97017 29.09477 58.84557 21.22021 66.72013\nJun 2018       43.39031 28.07408 58.70653 19.96616 66.81445\nJul 2018       38.90220 24.12853 53.67588 16.30782 61.49659\nAug 2018       44.42410 27.84663 61.00158 19.07104 69.77717\nSep 2018       59.30522 38.44478 80.16566 27.40193 91.20850\nOct 2018       39.87917 22.81850 56.93984 13.78712 65.97122\nNov 2018       60.58102 38.27853 82.88352 26.47230 94.68975\nDec 2018       61.45110 38.17748 84.72473 25.85717 97.04504\n\n\nCode\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\nCode\n#Furnishings-&gt;choose ETS\nfurnishings_ts &lt;- ts_list[[\"Furnishings\"]]\nets_model &lt;- ets(furnishings_ts)\nsummary(ets_model)\n\n\nETS(M,A,A) \n\nCall:\nets(y = furnishings_ts)\n\n  Smoothing parameters:\n    alpha = 0.0438 \n    beta  = 0.0437 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 15.4275 \n    b = -0.1137 \n    s = 13.3158 15.6269 -2.2962 10.1503 -5.0017 -2.448\n           -3.4406 -1.0728 -1.1262 -4.3688 -11.689 -7.6497\n\n  sigma:  0.2527\n\n     AIC     AICc      BIC \n338.8888 359.2888 370.6992 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.6402485 3.793384 2.884208 -0.8302416 16.2441 0.5352139\n                   ACF1\nTraining set 0.04613441\n\n\nCode\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\n\n\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\nJan 2018       21.37433 14.45350 28.29515 10.789837 31.95882\nFeb 2018       18.56644 12.52244 24.61044  9.322944 27.80994\nMar 2018       27.11574 18.26943 35.96205 13.586481 40.64500\nApr 2018       31.58782 21.22159 41.95404 15.734048 47.44158\nMay 2018       32.87189 21.95559 43.78819 16.176845 49.56693\nJun 2018       31.73384 20.95052 42.51717 15.242170 48.22551\nJul 2018       33.95710 22.18459 45.72960 15.952604 51.96159\nAug 2018       32.63192 20.83816 44.42569 14.594916 50.66893\nSep 2018       49.01546 31.92061 66.11032 22.871140 75.15979\nOct 2018       37.79884 23.38308 52.21460 15.751844 59.84584\nNov 2018       56.95159 36.44698 77.45621 25.592490 88.31070\nDec 2018       55.87232 34.96477 76.77986 23.896983 87.84765\n\n\nCode\nplot(ets_forecast, main = \"ETS Forecast for Furnishings (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Applying to all data (5b)\nIn this session, we first grouped the sub-categories into clusters based on key time-series features, including trend strength, seasonal strength, and random strength, using hierarchical clustering. Once the clusters were formed, we applied and evaluated multiple forecasting models—ARIMA, Holt-Winters, and ETS—on each sub-category within its respective cluster, comparing their accuracy metrics such as RMSE and MAPE. Based on the evaluation results, we selected the best-performing model for each sub-category and used it to forecast the future outcomes within a year, leveraging the clustering to enhance the accuracy and relevance of our predictions.\n\n\nCode\n#Cluster_Binders-&gt;Holt-Winters\ncluster_id &lt;- 1\nsubcategory &lt;- \"Binders\"\nhw_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast &lt;- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Binders (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_paper-&gt;ETS\ncluster_id &lt;- 2\nsubcategory &lt;- \"Paper\"\nets_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$ETS$Model\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_Furnishings-&gt;Holt-Winters\ncluster_id &lt;- 3\nsubcategory &lt;- \"Furnishings\"\nhw_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast &lt;- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Furnishings (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/SCDA.html#forecast-interpretation",
    "href": "projects/SCDA.html#forecast-interpretation",
    "title": "Supply Chain Data Analytics",
    "section": "3.6 Forecast interpretation",
    "text": "3.6 Forecast interpretation\nLorem Ipsum\n\n\n\nCode\n# Check for missing values\nmissing_values &lt;- colSums(is.na(data))\nprint(missing_values)  # Print missing values for reference\n# heat map\nlibrary(Amelia)\nmissmap(data, main = \"Missing Data Pattern\")\n#distribution of key variables\n#plot Quantity\nggplot(data, aes(x = Quantity)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\") +\n  labs(title = \"Distribution of Quantity\", x = \"Quantity\", y = \"Frequency\") +\n  theme_minimal()\n#plot sales\nggplot(data, aes(x = Sales)) +\n  geom_histogram(binwidth = 50, fill = \"tomato\") +\n  labs(title = \"Distribution of Sales\", x = \"Sales\", y = \"Frequency\") +\n  theme_minimal()\n# plot profit\nggplot(data, aes(x = Profit)) +\n  geom_histogram(binwidth = 10, fill = \"purple\") +\n  labs(title = \"Distribution of Profit\", x = \"Profit\", y = \"Frequency\") +\n  theme_minimal()\n# time based trends\ndata$Order_Date &lt;- as.Date(data$Order_Date, format = \"%Y-%m-%d\")  # Ensure date format\ntime_series &lt;- data %&gt;%\n  group_by(Order_Date) %&gt;%\n  summarize(total_sales = sum(Sales), total_profit = sum(Profit), total_quantity = sum(Quantity))\n\nggplot(time_series, aes(x = Order_Date)) +\n  geom_line(aes(y = total_sales, color = \"Sales\")) +\n  geom_line(aes(y = total_profit, color = \"Profit\")) +\n  geom_line(aes(y = total_quantity, color = \"Quantity\")) +\n  labs(title = \"Sales, Profit, and Quantity Over Time\", x = \"Date\", y = \"Value\") +\n  theme_minimal() +\n  scale_color_manual(name = \"Metrics\", values = c(\"Sales\" = \"blue\", \"Profit\" = \"green\", \"Quantity\" = \"red\"))\n\n#sales by category and sub category\ncategory_sales &lt;- data %&gt;%\n  group_by(Category, Sub_Category) %&gt;%\n  summarize(total_sales = sum(Sales))\n\nggplot(category_sales, aes(x = reorder(Sub_Category, -total_sales), y = total_sales, fill = Category)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sales by Category and Sub-Category\", x = \"Sub-Category\", y = \"Total Sales\") +\n  theme_minimal() +\n  coord_flip()\n\n#Outliers detection\n#Quantity\nggplot(data, aes(x = Category, y = Quantity)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Quantity by Category\", x = \"Category\", y = \"Quantity\")\n#sales\nggplot(data, aes(x = Category, y = Sales)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sales by Category\", x = \"Category\", y = \"Sales\")\n\n#profit\nggplot(data, aes(x = Category, y = Profit)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Profit by Category\", x = \"Category\", y = \"Profit\")\n#Geo visualization\n\nus_map &lt;- map_data(\"state\")\nif(\"State\" %in% colnames(data)) {\n  state_sales &lt;- data %&gt;%\n    group_by(State) %&gt;%\n    summarize(total_sales = sum(Sales))\n\n  # Convert state names to lowercase to match map data\n  state_sales$State &lt;- tolower(state_sales$State)\n\n  # Merge state sales data with map data\n  state_sales_map &lt;- merge(us_map, state_sales, by.x = \"region\", by.y = \"State\", all.x = TRUE)\n\n  # Plot sales by state\n  ggplot(state_sales_map, aes(long, lat, group = group, fill = total_sales)) +\n    geom_polygon(color = \"white\") +\n    scale_fill_continuous(low = \"lightblue\", high = \"darkblue\", na.value = \"gray90\") +\n    labs(title = \"Sales by State\", fill = \"Total Sales\") +\n    theme_void() +\n    coord_fixed(1.3)\n}\n\n# correlation matrix\nnumerical_data &lt;- data %&gt;% select(where(is.numeric))\n\ncor_matrix &lt;- cor(numerical_data, use = \"complete.obs\")\n\n# Convert the correlation matrix to a long format\ncor_data &lt;- as.data.frame(as.table(cor_matrix))\n\n# Plot the correlation matrix using ggplot2\nggplot(cor_data, aes(Var1, Var2, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n                       name=\"Correlation\") +\n  geom_text(aes(label = round(Freq, 2)), color = \"black\", size = 4) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                   size = 12, hjust = 1)) +\n  coord_fixed() +\n  labs(title = \"Correlation Matrix of Key Variables\", x = \"\", y = \"\")\n\n\n\n3.6.1 Forecasting??\n\n\nCode\n#Aggregate sales per month\nmonthly_sales &lt;- data %&gt;%\n  mutate(Month = floor_date(Order_Date, \"month\")) %&gt;%\n  group_by(Month) %&gt;%\n  summarize(total_sales = sum(Sales))\n#Convert to time series\nsales_ts &lt;- ts(monthly_sales$total_sales, frequency = 12, start = c(year(min(monthly_sales$Month)), month(min(monthly_sales$Month))))\n#Arima model\narima_model &lt;- auto.arima(sales_ts)\narima_forecast &lt;- forecast(arima_model, h = 12)\nautoplot(arima_forecast) + labs(title = \"ARIMA Forecast for Monthly Sales\")\n\n\n\n\n\n\n\n\n\nCode\n#Holts winter model\nhw_model &lt;- HoltWinters(sales_ts)\nhw_forecast &lt;- forecast(hw_model, h = 12)\nautoplot(hw_forecast) + labs(title = \"Holt-Winters Forecast for Monthly Sales\")\n\n\n\n\n\n\n\n\n\nCode\n# clustering for segmentation\nlibrary(cluster)\n#data clustering\nclustering_data &lt;- data %&gt;%\n  select(Sales, Quantity, Discount, Profit) %&gt;%\n  na.omit()\nset.seed(123)\nkmeans_model &lt;- kmeans(clustering_data, centers = 3)\ndata$Cluster &lt;- as.factor(kmeans_model$cluster)\n# visualize clustering result\nggplot(data, aes(x = Sales, y = Profit, color = Cluster)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"K-Means Clustering of Sales and Profit\", x = \"Sales\", y = \"Profit\") +\n  theme_minimal()",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html",
    "href": "projects/scda.html",
    "title": "Supply Chain Data Analytics",
    "section": "",
    "text": "3.1 Data selection\nWe analyze, forecast and interpret the Superstore sales provided by Tableau using different statistical and machine learning methods.\nThe dataset provided contains information about products, sales and profits of a fictitious US company. The dataset contains about 10,000 rows with 1,850 unique product names and 17 product subcategories, covering four consecutive years on a daily basis.\nWe describe our work in the PDF version. However, we would like to recommend reading our quarto manuscript here as it contains the relevant R code in the Article Notebook.",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html#data-pre-processing",
    "href": "projects/scda.html#data-pre-processing",
    "title": "Supply Chain Data Analytics",
    "section": "3.2 Data Pre-processing",
    "text": "3.2 Data Pre-processing\nThe superstore data set we selected is of high quality: At first glance (which needs to be verified during the visualization), the data appears to have been recorded regularly and without interruptions. There is no sign of a sudden structural change. Since the data are consumer products, it should contain both trends and seasonality. Nevertheless, we have included hypothetical steps to demonstrate our understanding of the data preprocessing procedure. In detail, we did:\n\n\nCode\n# Clear workspace\nrm(list = ls())\n# Function to load (and install if necessary) dependencies\ninstall_and_load &lt;- function(packages) {\n  install.packages(setdiff(packages, rownames(installed.packages())), dependencies = TRUE)\n  invisible(lapply(packages, require, character.only = TRUE))\n}\ninstall_and_load(c(\"tidyverse\", \"readxl\", \"ggplot2\", \"lubridate\", \"stats\", \"Amelia\",\"forecast\", \"tseries\", \"plotly\", \"stringr\", \"knitr\", \"kableExtra\"))\n\n\n\nImproved column names by removing whitespaces\nRemoved the Row_ID column as it can be inferred by it’s index\nRemoved all columns with a single unique value, as storing these would be redundant\nEnsured machine-readable date formats in yyyy-mm-dd as these usually differ per locale.\nEnsured proper decimal separators\nCalculated the number of missing values (both NA and empty string ““) per column.\n\n\n\nCode\n# Load the data\nsuppressWarnings({data &lt;- read_excel(\"data/sample_-_superstore.xls\")}) # The Postal code column is stored as 'text' but coerced to numeric, causing warnings which we suppress\n\n# Improve column names\ncolnames(data) &lt;- str_replace_all(colnames(data), \" \", \"_\")\ncolnames(data) &lt;- str_replace_all(colnames(data), \"-\", \"_\")\n\n# Remove the 'Row_ID' column as it can be inferred by it's index\ndata &lt;- subset(data, select = -`Row_ID`)\n\n# Remove all columns that have only one unique value, as storing these would be redundant\ndata &lt;- data[, sapply(data, function(col) length(unique(col)) &gt; 1)]\n\n# Ensure a machine-readable date format as these are usually horrible in excel files\ndata$Order_Date &lt;- as.Date(data$Order_Date, format = \"%Y-%m-%d\")\ndata$Ship_Date &lt;- as.Date(data$Ship_Date, format = \"%Y-%m-%d\")\n\n# The readxl package by default uses the correct decimal separator (as opposed to base R)\n\n# Calculate the number of missing values per column.\n# Origional dates and R date objects are in unix time, which return NA when compared to text (empty string). These dates are stored as 'double' datatype, Thus we check character columns for empty strings, and all columns for NA values. \nmissing_values &lt;- sapply(data, function(col) {\n  if (inherits(col, \"Date\")) {\n    sum(is.na(col))\n  } else if (is.character(col)) {\n    sum(is.na(col) | col == \"\")\n  } else {\n    sum(is.na(col))\n  }\n})\n\n# sum(missing_values) returns 0!\n\n# Optionally, print the missing values as a nice table\nmissing_values_table &lt;- data.frame(\n  Column = names(missing_values),\n  Missing_or_Empty = missing_values\n)\n# Note that there are no missing values, thus we do not print them\n# kable(missing_values_table, caption = \"Missing or Empty Values in Columns\", format = \"pipe\")\n\n\nrm(missing_values, missing_values_table)\n\n\nAfter these steps (and transposing the table for better document formatting), the data looks as follows:\n\n\nCode\nkable(t(head(data, 3)), caption = \"First 3 Rows of the Data (Transposed)\", format = \"markdown\")\n\n\n\nFirst 3 Rows of the Data (Transposed)\n\n\n\n\n\n\n\n\nOrder_ID\nCA-2016-152156\nCA-2016-152156\nCA-2016-138688\n\n\nOrder_Date\n2016-11-08\n2016-11-08\n2016-06-12\n\n\nShip_Date\n2016-11-11\n2016-11-11\n2016-06-16\n\n\nShip_Mode\nSecond Class\nSecond Class\nSecond Class\n\n\nCustomer_ID\nCG-12520\nCG-12520\nDV-13045\n\n\nCustomer_Name\nClaire Gute\nClaire Gute\nDarrin Van Huff\n\n\nSegment\nConsumer\nConsumer\nCorporate\n\n\nCity\nHenderson\nHenderson\nLos Angeles\n\n\nState\nKentucky\nKentucky\nCalifornia\n\n\nPostal_Code\n42420\n42420\n90036\n\n\nRegion\nSouth\nSouth\nWest\n\n\nProduct_ID\nFUR-BO-10001798\nFUR-CH-10000454\nOFF-LA-10000240\n\n\nCategory\nFurniture\nFurniture\nOffice Supplies\n\n\nSub_Category\nBookcases\nChairs\nLabels\n\n\nProduct_Name\nBush Somerset Collection Bookcase\nHon Deluxe Fabric Upholstered Stacking Chairs, Rounded Back\nSelf-Adhesive Address Labels for Typewriters by Universal\n\n\nSales\n261.96\n731.94\n14.62\n\n\nQuantity\n2\n3\n2\n\n\nDiscount\n0\n0\n0\n\n\nProfit\n41.9136\n219.5820\n6.8714\n\n\n\n\n\nWe did not find any missing values, confirming the quality of the data set. There is some more processing to do, for instance the removal of outliers. However, by doing so we impose our own assumptions on the data. Let’s start by evaluating the descriptive statistics of our data and check if further processing is required.\n\n\nCode\ndescriptive_statistics &lt;- function(column) {\n  if (is.numeric(column)) {\n    stats &lt;- list(\n      Min = min(column, na.rm = TRUE), # Note that handling NA values increases robustness (and I copied the funciton from some of my earlier work)\n      Max = max(column, na.rm = TRUE),\n      Mean = mean(column, na.rm = TRUE),\n      Median = median(column, na.rm = TRUE),\n      StdDev = sd(column, na.rm = TRUE)\n    )\n  } else if (inherits(column, \"Date\")) {\n    stats &lt;- list(\n      Earliest = format(min(column, na.rm = TRUE), \"%Y-%m-%d\"),\n      Latest = format(max(column, na.rm = TRUE), \"%Y-%m-%d\")\n    )\n  } else if (is.character(column)) {\n    stats &lt;- list(\n      Unique = length(unique(column)),\n      Mode = names(sort(table(column), decreasing = TRUE)[1])\n    )\n  } else {\n    stats &lt;- NULL\n  }\n  return(stats)\n}\n\n# Call function on dataframe\ndescriptive_stats &lt;- lapply(data, descriptive_statistics)\n\n# Separate to tables dependent on data type\nnumeric_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.numeric(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ndate_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (inherits(data[[col_name]], \"Date\")) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\ncharacter_stats &lt;- as.data.frame(do.call(rbind, lapply(names(data), function(col_name) {\n  if (is.character(data[[col_name]])) {\n    c(Column = col_name, descriptive_stats[[col_name]])\n  }\n})), stringsAsFactors = FALSE)\n\n\n\n\nCode\nkable(\n  numeric_stats,\n  caption = \"Descriptive Statistics for Numeric Columns\",\n  format = \"pipe\")\n\n\n\nDescriptive Statistics for Numeric Columns\n\n\nColumn\nMin\nMax\nMean\nMedian\nStdDev\n\n\n\n\nPostal_Code\n1040\n99301\n55190.38\n56430.5\n32063.69\n\n\nSales\n0.444\n22638.48\n229.858\n54.49\n623.2451\n\n\nQuantity\n1\n14\n3.789574\n3\n2.22511\n\n\nDiscount\n0\n0.8\n0.1562027\n0.2\n0.206452\n\n\nProfit\n-6599.978\n8399.976\n28.6569\n8.6665\n234.2601\n\n\n\n\n\nCode\nkable(\n  date_stats,\n  caption = \"Descriptive Statistics for Date Columns\",\n  format = \"pipe\")\n\n\n\nDescriptive Statistics for Date Columns\n\n\nColumn\nEarliest\nLatest\n\n\n\n\nOrder_Date\n2014-01-03\n2017-12-30\n\n\nShip_Date\n2014-01-07\n2018-01-05\n\n\n\n\n\nWe inspect the orders with the lowest and highest Sales amount (in USD). The most expensive orders were professional printers, cameras and teleconferencing units with high unit prices. The orders with the lowest sales amount were often binders and had a high Discount rate.\nInterestingly there are orders with a negative profit. They typically have high Discount rates and often concern the same item, such as the “Cubify CubeX 3D Printer Triple Head Print”. The orders with a negative Profit were often part of a larger order (for instance CA-2016-108196), and placed by customers with multiple orders. We suspect these negative Profit’s to be caused by items of lower quality that receive discounts, general discount codes, or volume discounts. However, due to the high discounts especially on orders with negative profit, we assume these to be valid orders.\n** Some negative profit products **\nIn figure x we plotted the quantities of the most sold products. Unfortunately, the sold quantities of individual products were too low to determine any meaningful trends.\n\n\nCode\n# Optionally: print top 10 sale quantity barplot\n# # Sum of Quantity for top products\n# top_products &lt;- data %&gt;%\n#   group_by(Product_Name) %&gt;%\n#   summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%\n#   arrange(desc(total_quantity)) %&gt;%\n#   slice_head(n = 10) %&gt;% \n#   mutate(ProdName8 = substr(Product_Name, 1, 8)) # Truncate product names to the first 8 characters. Long names mess up formatting\n# \n# # Plot\n# ggplot(top_products, aes(x = reorder(ProdName8, -total_quantity), y = total_quantity)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product ID\",\n#        y = \"Total Quantity\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Aggregate quantity by Product Name and Order Date to create a time series\ntime_series_data &lt;- data %&gt;%\n  group_by(Product_Name, Order_Date) %&gt;%\n  summarize(total_quantity = sum(Quantity, na.rm = TRUE)) %&gt;%\n  ungroup()\n# Filter for the top products by total quantity sold (adjust as needed)\ntop_products &lt;- time_series_data %&gt;%\n  group_by(Product_Name) %&gt;%\n  summarize(total_quantity = sum(total_quantity)) %&gt;%\n  arrange(desc(total_quantity)) %&gt;%\n  slice_head(n = 10)  # Select top 10 products\n\n# Filter the time-series data for only these top products\nfiltered_time_series_data &lt;- time_series_data %&gt;%\n  filter(Product_Name %in% top_products$Product_Name) %&gt;%\n  mutate(ProdName10 = substr(Product_Name, 1, 10)) # Product names can be quite long and mess up layouts. Lets only plot the first 10 chars.\n\n# Here we do some special plotting. We want to show the plot with only one selected line by default, but make sure that the other 9 top sold products can be selected. We first create the ggplotly object, and than modify the visibility of the traces\n\n\n\n\nCode\n# Plot interactive figure in html, plot ggplot2 in pdf:\n\n# Creating the ggplotly object\np_ly &lt;- ggplotly(ggplot(filtered_time_series_data, aes(x = Order_Date, y = total_quantity, color = ProdName10)) +\n  geom_line(size = 1) +\n  labs(title = \"Quantity Sold Over Time per Product\",\n       x = \"Order Date\",\n       y = \"Quantity Sold\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\") +\n  scale_color_discrete(name = \"Product Name\"))\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nCode\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible &lt;- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n\n\n\n\nFigure X Sale quantity of the most popular products\n\n\nOur proposed workaround is to aggregate Product_Name by Sub_Category, and treat it as a single product for the rest of the assignment, which we plotted in figure X.\n\n\nCode\n# Bar plots\n\n# # Count frequency of top 20 products\n# top_products &lt;- data %&gt;%\n#   count(Product_Name, sort = TRUE) %&gt;%\n#   top_n(20, n) %&gt;%\n#   mutate(ProdName8 = substr(Product_Name, 1, 8))\n# \n# # Plot!\n# ggplot(top_products, aes(x = reorder(`ProdName8`, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Top 20 Most Sold Products\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n# \n# Count frequency of top 20 products\ntop_categories &lt;- data %&gt;%\n  count(Sub_Category, sort = TRUE)\n# \n# # Plot!\n# ggplot(top_categories, aes(x = reorder(Sub_Category, -n), y = n)) +\n#   geom_bar(stat = \"identity\", fill = \"steelblue\") +\n#   labs(title = \"Sub_Categories sorted\",\n#        x = \"Product Name\",\n#        y = \"Quantity sold\") +\n#   theme_minimal() +\n#   coord_flip()\n\n# Find top 10 most sold product names\ntop_10_categories &lt;- top_categories$Sub_Category[0:10]\n\n# Filter the data for  top 10 products\ntop_10_data &lt;- data %&gt;% filter(Sub_Category %in% top_10_categories)\n\n# calculate sales per month\ntop_10_data &lt;- top_10_data %&gt;%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each sub-category\ntop_10_data_aggregated &lt;- top_10_data %&gt;%\n  group_by(Month, Sub_Category) %&gt;%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Some special interactive plot formatting (see previous plot)\np_ly &lt;- ggplotly(ggplot(top_10_data_aggregated, aes(x = Month, y = Sales_Count, color = Sub_Category, group = Sub_Category)) +\n    geom_line(size = 1) +\n    geom_point(size = 2) +\n    labs(title = \"Monthly Sales for the Top 3 Most Sold Products\",\n         x = \"Month\",\n         y = \"Sales Count\",\n         color = \"Product Name\") +\n    theme_minimal())\n\n# Modify the visibility of traces\nfor (i in seq_along(p_ly$x$data)) {\n  if (i == 1) {\n    p_ly$x$data[[i]]$visible &lt;- TRUE  # Make the first trace visible\n  } else {\n    p_ly$x$data[[i]]$visible &lt;- \"legendonly\"  # Hide the rest\n  }\n}\n\n# Plot\np_ly\n\n\n\n\n\n\nThis aggregated Quantity starts to show trends and seasonality, and is much more useful to base predictions on! We will use these aggregated sub-categories for the rest of the assignment.\nTo properly finish our data preprocessing we ran some statistics on Quantity aggregated by Sub_Category. Table x contains some descriptive statistics.\n\n\nCode\nlibrary(dplyr)\nlibrary(kableExtra)\n\n# Summarize the data\noutlier_summary &lt;- data %&gt;%\n  group_by(Sub_Category) %&gt;%\n  summarize(\n    Min = round(min(Quantity), 2),\n    Mean = round(mean(Quantity), 2),\n    Max = round(max(Quantity), 2),\n    Sd = round(sd(Quantity), 2),\n    CI_lower = round(Mean - 1.96 * (Sd / sqrt(n())), 2),\n    CI_upper = round(Mean + 1.96 * (Sd / sqrt(n())), 2),\n    .groups = \"drop\"\n  )\n\n# Output tables\nkable(\n  outlier_summary,\n  caption = \"Statistics for Sub_Category quantity\",\n  format = \"pipe\")\n\n\n\nStatistics for Sub_Category quantity\n\n\nSub_Category\nMin\nMean\nMax\nSd\nCI_lower\nCI_upper\n\n\n\n\nAccessories\n1\n3.84\n14\n2.28\n3.68\n4.00\n\n\nAppliances\n1\n3.71\n14\n2.12\n3.52\n3.90\n\n\nArt\n1\n3.77\n14\n2.13\n3.62\n3.92\n\n\nBinders\n1\n3.92\n14\n2.29\n3.80\n4.04\n\n\nBookcases\n1\n3.81\n13\n2.28\n3.51\n4.11\n\n\nChairs\n1\n3.82\n14\n2.28\n3.64\n4.00\n\n\nCopiers\n1\n3.44\n9\n1.83\n3.01\n3.87\n\n\nEnvelopes\n1\n3.57\n9\n2.05\n3.32\n3.82\n\n\nFasteners\n1\n4.21\n14\n2.41\n3.89\n4.53\n\n\nFurnishings\n1\n3.72\n14\n2.16\n3.58\n3.86\n\n\nLabels\n1\n3.85\n14\n2.35\n3.61\n4.09\n\n\nMachines\n1\n3.83\n11\n2.17\n3.43\n4.23\n\n\nPaper\n1\n3.78\n14\n2.23\n3.66\n3.90\n\n\nPhones\n1\n3.70\n14\n2.19\n3.56\n3.84\n\n\nStorage\n1\n3.73\n14\n2.19\n3.58\n3.88\n\n\nSupplies\n1\n3.41\n10\n1.84\n3.15\n3.67\n\n\nTables\n1\n3.89\n13\n2.45\n3.62\n4.16\n\n\n\n\n\nThe statistics for Quantity aggregated by Sub_Category looks valid. We can visualize it as histogram and check for anomalies. Figure y contains histograms of Quantity per Sub_Category.\n\n\nCode\nsub_categories &lt;- unique(data$Sub_Category)\n\np &lt;- plot_ly()\nfor (i in seq_along(sub_categories)) {\n  sub &lt;- sub_categories[i]\n  subset_data &lt;- data %&gt;% filter(Sub_Category == sub)\n  p &lt;- add_trace(\n    p,\n    x = subset_data$Quantity,\n    type = \"histogram\",\n    name = sub,\n    visible = ifelse(i == 1, TRUE, FALSE)\n  )\n}\n\n# We add a drop down menu for Sub_Category as toggling visibility in default ggplot2 adds the histograms up. Instead we want to be able to show each histogram seperately. \ndropdown_buttons &lt;- lapply(seq_along(sub_categories), function(i) {\n  list(\n    method = \"update\",\n    args = list(\n      list(visible = lapply(seq_along(sub_categories), function(j) j == i)),\n      list(xaxis = list(title = \"Quantity\", autorange = TRUE), \n           yaxis = list(title = \"Frequency\", autorange = TRUE))\n    ),\n    label = sub_categories[i]\n  )\n})\n\n# Style drop down layout\np &lt;- p %&gt;%\n  layout(\n    title = \"Distribution of Quantity Sold per Order by Sub-Category\",\n    xaxis = list(title = \"Quantity\"),\n    yaxis = list(title = \"Frequency\"),\n    showlegend = FALSE,  # Drop down instead of legend\n    updatemenus = list(\n      list(\n        type = \"dropdown\",\n        buttons = dropdown_buttons,\n        direction = \"down\",\n        x = 0.99,\n        y = 0.99,\n        showactive = TRUE,\n        xanchor = \"left\",\n        yanchor = \"top\"\n      )\n    )\n  )\np\n\n\n\n\n\n\nThe histograms show that the quantities a right-skewed distributed. This is to be expected since most orders contain only a small number of items. We will not remove the outliers with large quantities since they appear valid..",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html#data-visualization",
    "href": "projects/scda.html#data-visualization",
    "title": "Supply Chain Data Analytics",
    "section": "3.3 Data Visualization",
    "text": "3.3 Data Visualization",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html#forecasting-method-evaluation",
    "href": "projects/scda.html#forecasting-method-evaluation",
    "title": "Supply Chain Data Analytics",
    "section": "3.4 Forecasting Method Evaluation",
    "text": "3.4 Forecasting Method Evaluation\n\n3.4.1 Forecasting top 3 product categories (4a)\nLet’s forecast sold quantities for the three most sold sub-categories:\nThe steps taken for data preparation were:\n\nIdentifying Top Subcategories: The top three subcategories are selected from our dataset based on their sold quantities. The top three were: Binders, furnishing and paper.\nThe sold quantities are aggregated monthly to create a time series object which we can use in the forecasting.\nA KPSS showed that the data is non stationary. First-order differencing is applied to transform the data from non-stationary to stationary. The KPSS results in a p-value &gt;0.05 showing the stationarity.\n\n\n\nCode\n# Find top 3 most sold product names\ntop_categories &lt;- data %&gt;%\n  group_by(Sub_Category) %&gt;%\n  summarise(Total_Quantity = sum(Quantity)) %&gt;%\n  arrange(desc(Total_Quantity))\ntop_3_subcategories &lt;- top_categories$Sub_Category[0:3]\n\n# Filter the data for  top 3 products\ntop_3_data &lt;- data %&gt;% filter(Sub_Category %in% top_3_subcategories)\n\n# calculate sales per month\ntop_3_data &lt;- top_3_data %&gt;%\n  mutate(Month = floor_date(Order_Date, unit = \"month\"))\n\n# Aggregate data by month for each product\ntop_3_data_aggregated &lt;- top_3_data %&gt;%\n  group_by(Month, Sub_Category) %&gt;%\n  summarise(Sales_Count = n(), .groups = 'drop')\n\n# Create a time series object for each product\nts_data &lt;- top_3_data_aggregated %&gt;%\n  pivot_wider(names_from = Sub_Category, values_from = Sales_Count, values_fill = 0) %&gt;%\n  select(-Month) %&gt;%\n  as.matrix()\n\n# Create a time series object\nts_data &lt;- ts(ts_data, start = c(2014, 1), end = c(2017, 12), frequency = 12)\n\n# Create a time series list for each subcategory\nts_list &lt;- list()\n\nfor (subcategory in top_3_subcategories) {\n  # Filter data for the subcategory\n  subcategory_data &lt;- top_3_data_aggregated %&gt;% filter(Sub_Category == subcategory)\n\n  # Create a time series object (assuming monthly data from January 2014 to December 2017)\n  ts_list[[subcategory]] &lt;- ts(subcategory_data$Sales_Count,\n                                start = c(2014, 1),\n                                end = c(2017, 12),\n                                frequency = 12)\n}\n\n#### 4 A\n# Step 4: Apply forecasting methods to the top 3 sub-categories\nforecast_results &lt;- list()  # Store results\n\nfor (subcategory in names(ts_list)) {\n  ts_current &lt;- ts_list[[subcategory]]\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size &lt;- floor(0.7 * length(ts_current))\n  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model &lt;- auto.arima(train_ts)\n  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model &lt;- HoltWinters(train_ts)\n  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model &lt;- ets(train_ts)\n  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] &lt;- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n\n# For formatting, we ommitted almost all output. You can uncomment the code and check your output if you like.\n\n# # Step 5: Print results\n#  for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n\n\nThree models are applied to each subcategory to forecast it. The models we use are: ARIMA, Holt-Winters and ETS. We have chosen these models because of their level of suitability for discrete time series data with all different levels of trend and seasonality. To evaluate the methods and its effectiveness , the data is split into a training set (70%) and testing set (30%).\nTo assess the results, we use the following performance metrics: ME, RMSE, MAE and MAPE. They are calculated for the training and testing phases of the forecast.\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE if you compare it to the other models.\n\n\nCode\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current &lt;- ts_list[[subcategory]]\n  train_size &lt;- floor(0.7 * length(ts_current))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\noptions(warn = -1)\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast, main = paste(\"ARIMA Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast, main = paste(\"Holt-Winters Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast, main = paste(\"ETS Forecast for\", subcategory))\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n#more stationary tests\n# Perform KPSS Test for the top 3 subcategories\n# top_3_subcategories &lt;- top_categories$Sub_Category[0:3]\n# \n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(ts_list)) {\n#     ts_current &lt;- ts_list[[subcategory]]\n#     cat(\"\\nKPSS Test for Sub-Category:\", subcategory, \"\\n\")\n#     print(kpss.test(ts_current))\n#   } else {\n#     cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n#   }\n# }\n\n#Because the all the 3 subcategory are non stationary because of a P value which is &lt;=0.05 we need to use differencing\n# Apply differencing to each of the top 3 subcategories\ndifferenced_series &lt;- list()\n\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(ts_list)) {\n    ts_current &lt;- ts_list[[subcategory]]  # Get the time series for the subcategory\n    ts_diff &lt;- diff(ts_current, differences = 1)  # Apply first-order differencing\n    differenced_series[[subcategory]] &lt;- ts_diff  # Store the differenced series\n\n    # Recheck stationarity with KPSS test\n    #cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_diff))\n  } else {\n    #cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n  }\n}\n\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\n    KPSS Test for Level Stationarity\n\ndata:  ts_diff\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n\n\n\n\nCode\n# Time differenced plots\n# # Now P value is larger then 0.05 so we have stationary data\n# # Plot the differenced series for each subcategory\n# for (subcategory in top_3_subcategories) {\n#   if (subcategory %in% names(differenced_series)) {\n#     ts_diff &lt;- differenced_series[[subcategory]]\n#     cat(\"\\nPlotting Differenced Series for Sub-Category:\", subcategory, \"\\n\")\n#     plot(ts_diff, main = paste(\"Differenced Series for Sub-Category:\", subcategory),\n#          ylab = \"Differenced Values\", xlab = \"Time\")\n#   }\n# }\n\n\n# Combine differenced series plots for all top subcategories\ncombined_diff_plot &lt;- function(differenced_series, top_3_subcategories) {\n  plot(NULL, xlim = range(time(differenced_series[[top_3_subcategories[1]]])), \n       ylim = range(sapply(differenced_series[top_3_subcategories], range, na.rm = TRUE)),\n       xlab = \"Time\", ylab = \"Differenced Values\",\n       main = \"Differenced Series for Top Subcategories\")\n  colors &lt;- c(\"blue\", \"green\", \"purple\")\n  for (i in seq_along(top_3_subcategories)) {\n    subcategory &lt;- top_3_subcategories[i]\n    if (subcategory %in% names(differenced_series)) {\n      ts_diff &lt;- differenced_series[[subcategory]]\n      lines(ts_diff, col = colors[i], lwd = 2, lty = i)\n    }\n  }\n  legend(\"topright\", legend = top_3_subcategories, \n         col = colors, lty = 1:length(top_3_subcategories), lwd = 2)\n}\ncombined_diff_plot(differenced_series, top_3_subcategories)\n\n\n\n\n\n\n\n\n\n\n\nCode\n#NEW FORECASTING FOR 4A with stationary data\n# Step 4: Apply forecasting methods to the differenced top 3 sub-categories\nforecast_results &lt;- list()  # Store results\n\nfor (subcategory in names(differenced_series)) {\n  ts_current &lt;- differenced_series[[subcategory]]  # Use the differenced series\n\n  # Split the data into training and validation sets (70% training, 30% testing)\n  train_size &lt;- floor(0.7 * length(ts_current))\n  train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n  # 1. ARIMA\n  arima_model &lt;- auto.arima(train_ts)\n  arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n  arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n  # 2. Holt-Winters\n  hw_model &lt;- HoltWinters(train_ts)\n  hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n  hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n  # 3. ETS\n  ets_model &lt;- ets(train_ts)\n  ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n  ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n  # Store results\n  forecast_results[[subcategory]] &lt;- list(\n    ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n    HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n    ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n  )\n}\n\n# # Step 5: Print results\n# for (subcategory in names(forecast_results)) {\n#   cat(\"\\n\\nResults for Sub_Category:\", subcategory, \"\\n\")\n# \n#   cat(\"\\nARIMA Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ARIMA$Accuracy)\n# \n#   cat(\"\\nHolt-Winters Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$HoltWinters$Accuracy)\n# \n#   cat(\"\\nETS Accuracy:\\n\")\n#   print(forecast_results[[subcategory]]$ETS$Accuracy)\n# }\n\n# # Step 6: Visualization of Forecasts\n# for (subcategory in names(forecast_results)) {\n#   plot(forecast_results[[subcategory]]$ARIMA$Forecast,\n#        main = paste(\"ARIMA Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$HoltWinters$Forecast,\n#        main = paste(\"Holt-Winters Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# \n#   plot(forecast_results[[subcategory]]$ETS$Forecast,\n#        main = paste(\"ETS Forecast for\", subcategory),\n#        ylab = \"Differenced Values\", xlab = \"Time\")\n#   lines(test_ts, col = \"red\", lty = 2)\n# }\n\n# Lets plot the results\nfor (subcategory in names(forecast_results)) {\n  ts_current &lt;- ts_list[[subcategory]]\n  train_size &lt;- floor(0.7 * length(ts_current))\n  test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n  arima_forecast &lt;- forecast_results[[subcategory]]$ARIMA$Forecast\n  hw_forecast &lt;- forecast_results[[subcategory]]$HoltWinters$Forecast\n  ets_forecast &lt;- forecast_results[[subcategory]]$ETS$Forecast\n  \n  # Combined plot\n  plot(arima_forecast$mean, col = \"blue\", lwd = 2, \n       ylim = range(c(arima_forecast$mean, hw_forecast$mean, ets_forecast$mean, test_ts)),\n       main = paste(\"Combined Forecasts for\", subcategory, \"before differencing\"),\n       xlab = \"Time\", ylab = \"Forecasted Values\")\n  lines(test_ts, col = \"red\", lty = 2, lwd = 2)\n  lines(hw_forecast$mean, col = \"green\", lwd = 2)\n  lines(ets_forecast$mean, col = \"purple\", lwd = 2)\n  legend(\"topleft\", legend = c(\"ARIMA\", \"Holt-Winters\", \"ETS\", \"Test Data\"),\n         col = c(\"blue\", \"green\", \"purple\", \"red\"), lty = c(1, 1, 1, 2), lwd = 2)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# final KPSS test\n\n# Perform KPSS Test for the differenced series in the top 3 subcategories\nfor (subcategory in top_3_subcategories) {\n  if (subcategory %in% names(differenced_series)) {\n    ts_current &lt;- differenced_series[[subcategory]]  # Get the differenced series\n    cat(\"\\nKPSS Test for Differenced Sub-Category:\", subcategory, \"\\n\")\n    print(kpss.test(ts_current))\n  } else {\n    cat(\"\\nSub-Category not found in differenced_series:\", subcategory, \"\\n\")\n  }\n}\n\n\n\nKPSS Test for Differenced Sub-Category: Binders \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.10182, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Paper \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.061982, Truncation lag parameter = 3, p-value = 0.1\n\n\nKPSS Test for Differenced Sub-Category: Furnishings \n\n    KPSS Test for Level Stationarity\n\ndata:  ts_current\nKPSS Level = 0.098438, Truncation lag parameter = 3, p-value = 0.1\n\n\nCode\n# now they are all 0.1\n\n\nAs we can see on the forecasting results ARIMA performed well for binders. We can state this because of the lowest RMSE. - ARIMA Binders: - Forecasting results Binders:\nFor the subcategory furnishings we can see that the ETS forecasting method is the most stable across the training and testing phase. - ETS furnishings - Forecasting results Furnishings:\nFor the last subcategory and product paper the ETS model is again the most consistent, comparing the statistics for training and test set. The high variability in the test data leads to larger forecasting errors in all the 3 models.\n\nETS Furnishings:\nForecasting results:\n\nResidual Diagnostics: - The checks show no real autocorrelation for ARIMA models. Which indicates a good fitting forecast.\n\n\n3.4.2 Conclusion (4a)\nThe most effective model is not the same in all the subcategories. Each model was validated based on its ability to capture seasonality and trend. ARIMA performed better for Binders, while ETS performed better for Furnishings and Paper.\n\n\n3.4.3 Clustering (4b)\n\n\nCode\n# 4B\n# 4B: Group Products into Clusters and Apply Forecasting Techniques\n# Step 1: Extract Time-Series Features for Clustering\ntime_series_features &lt;- data.frame()\n\nfor (subcategory in names(ts_list)) {\n  ts_current &lt;- ts_list[[subcategory]]\n\n  # Decompose the time series to extract features\n  decomposition &lt;- decompose(ts_current)\n  trend_strength &lt;- var(decomposition$trend, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  seasonal_strength &lt;- var(decomposition$seasonal, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n  random_strength &lt;- var(decomposition$random, na.rm = TRUE) / var(ts_current, na.rm = TRUE)\n\n  # Store extracted features\n  time_series_features &lt;- rbind(time_series_features,\n                                data.frame(SubCategory = subcategory,\n                                           TrendStrength = trend_strength,\n                                           SeasonalStrength = seasonal_strength,\n                                           RandomStrength = random_strength))\n}\n\n# Step 2: Normalize the Features for Clustering\ntime_series_features_scaled &lt;- time_series_features %&gt;%\n  select(-SubCategory) %&gt;%\n  scale()\n\n# verify rows\n# nrow(time_series_features_scaled)\n\n# Step 3: Perform K-Means Clustering\n# Determine the number of clusters dynamically\nk &lt;- min(3, nrow(time_series_features_scaled))  # Set k to the smaller of 3 or the number of rows\n# Hierarchical Clustering\ndistance_matrix &lt;- dist(time_series_features_scaled)  # Calculate distance matrix\nhc &lt;- hclust(distance_matrix)  # Perform hierarchical clustering\ntime_series_features$Cluster &lt;- cutree(hc, k = k)  # Cut tree into 'k' clusters\n# Add cluster information to the original data\ntime_series_features$Cluster &lt;- cutree(hc, k = k)\n\n# Step 4: Apply Forecasting Techniques to Each Cluster\nforecast_results_by_cluster &lt;- list()\n\nfor (cluster_id in unique(time_series_features$Cluster)) {\n  # Get subcategories in the current cluster\n  subcategories_in_cluster &lt;- time_series_features$SubCategory[time_series_features$Cluster == cluster_id]\n\n  # Initialize storage for cluster results\n  cluster_forecast_results &lt;- list()\n\n  for (subcategory in subcategories_in_cluster) {\n    if (subcategory %in% names(ts_list)) {\n      ts_current &lt;- ts_list[[subcategory]]  # Access the time series\n\n      # Split the data into training and validation sets (70% training, 30% testing)\n      train_size &lt;- floor(0.7 * length(ts_current))\n      train_ts &lt;- window(ts_current, end = c(2014 + (train_size - 1) %/% 12, (train_size - 1) %% 12 + 1))\n      test_ts &lt;- window(ts_current, start = c(2014 + train_size %/% 12, train_size %% 12 + 1))\n\n      # 1. ARIMA\n      arima_model &lt;- auto.arima(train_ts)\n      arima_forecast &lt;- forecast(arima_model, h = length(test_ts))\n      arima_accuracy &lt;- accuracy(arima_forecast, test_ts)\n\n      # 2. Holt-Winters\n      hw_model &lt;- HoltWinters(train_ts)\n      hw_forecast &lt;- forecast(hw_model, h = length(test_ts))\n      hw_accuracy &lt;- accuracy(hw_forecast, test_ts)\n\n      # 3. ETS\n      ets_model &lt;- ets(train_ts)\n      ets_forecast &lt;- forecast(ets_model, h = length(test_ts))\n      ets_accuracy &lt;- accuracy(ets_forecast, test_ts)\n\n      # Store results for the subcategory\n      cluster_forecast_results[[subcategory]] &lt;- list(\n        ARIMA = list(Model = arima_model, Forecast = arima_forecast, Accuracy = arima_accuracy),\n        HoltWinters = list(Model = hw_model, Forecast = hw_forecast, Accuracy = hw_accuracy),\n        ETS = list(Model = ets_model, Forecast = ets_forecast, Accuracy = ets_accuracy)\n      )\n    } else {\n      cat(\"\\nSub-Category not found in ts_list:\", subcategory, \"\\n\")\n    }\n  }\n\n  # Store results for the cluster\n  forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]] &lt;- cluster_forecast_results\n}\n\n# Step 5: Print Forecasting Accuracy for Each Cluster\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cat(\"\\n\\nResults for\", cluster_id, \"\\n\")\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nSub-Category:\", subcategory, \"\\n\")\n\n    cat(\"\\nARIMA Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ARIMA$Accuracy)\n\n    cat(\"\\nHolt-Winters Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$HoltWinters$Accuracy)\n\n    cat(\"\\nETS Accuracy:\\n\")\n    print(cluster_results[[subcategory]]$ETS$Accuracy)\n  }\n}\n\n\n\n\nResults for Cluster_1 \n\nSub-Category: Binders \n\nARIMA Accuracy:\n                    ME      RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.7706014  4.643476 2.982256  0.6865304 12.91204 0.4854835\nTest set     5.9407398 10.783528 7.616473 10.3681817 17.32927 1.2398909\n                   ACF1 Theil's U\nTraining set 0.04429472        NA\nTest set     0.04929320 0.3573866\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE      MASE\nTraining set 0.8668058 5.215491 3.789990 -0.4040374 15.36545 0.6169751\nTest set     2.2473496 8.712049 6.243226  0.1597635 16.02219 1.0163391\n                     ACF1 Theil's U\nTraining set -0.389045966        NA\nTest set     -0.001830843 0.2777843\n\nETS Accuracy:\n                   ME      RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.743354  4.712561 3.656409  1.358692 15.41708 0.5952293\nTest set     7.439784 12.216161 8.825094 10.667088 20.93561 1.4366433\n                   ACF1 Theil's U\nTraining set -0.2225537        NA\nTest set      0.0608554 0.3767484\n\n\nResults for Cluster_2 \n\nSub-Category: Paper \n\nARIMA Accuracy:\n                    ME      RMSE       MAE        MPE     MAPE      MASE\nTraining set  1.117384  6.309464  3.827714   1.702165 14.32157 0.5468163\nTest set     -9.014128 12.230774 10.375521 -30.103886 31.89519 1.4822173\n                     ACF1 Theil's U\nTraining set -0.007064333        NA\nTest set      0.108516273 0.6984566\n\nHolt-Winters Accuracy:\n                     ME     RMSE       MAE        MPE     MAPE      MASE\nTraining set   1.509544  7.27986  4.854547   3.325281 16.41309 0.6935067\nTest set     -12.036865 14.48061 13.347292 -39.791842 41.51609 1.9067560\n                    ACF1 Theil's U\nTraining set -0.02710216        NA\nTest set      0.10006713  0.845232\n\nETS Accuracy:\n                     ME      RMSE      MAE         MPE     MAPE      MASE\nTraining set 0.60941117  6.204628 4.474990 -1.66197953 18.73240 0.6392842\nTest set     0.08500424 11.304488 8.247017 -0.08462612 20.47598 1.1781453\n                    ACF1 Theil's U\nTraining set 0.005205508        NA\nTest set     0.341519997  0.582549\n\n\nResults for Cluster_3 \n\nSub-Category: Furnishings \n\nARIMA Accuracy:\n                    ME     RMSE      MAE       MPE     MAPE      MASE\nTraining set 0.0050974 3.873555 2.567868 -6.624673 20.20958 0.5559302\nTest set     3.9523810 7.782677 6.238095 10.720079 22.75340 1.3505155\n                    ACF1 Theil's U\nTraining set -0.20160077        NA\nTest set     -0.03570035 0.6102339\n\nHolt-Winters Accuracy:\n                    ME     RMSE      MAE      MPE     MAPE      MASE\nTraining set 0.9137655 4.164677 3.475419 7.490317 24.75583 0.7524103\nTest set     3.6371987 7.200985 5.673333 9.724318 20.50134 1.2282474\n                    ACF1 Theil's U\nTraining set -0.43317305        NA\nTest set      0.01804785 0.5689788\n\nETS Accuracy:\n                    ME     RMSE      MAE        MPE     MAPE     MASE\nTraining set 0.7370579 3.466690 2.851745  0.4301648 20.85220 0.617388\nTest set     6.0973038 8.354163 6.690832 20.7276401 23.55317 1.448531\n                   ACF1 Theil's U\nTraining set -0.2087083        NA\nTest set      0.3729315 0.7455977\n\n\nCode\n## Step 6: Visualize the Clusters\nlibrary(ggplot2)\n\nggplot(time_series_features, aes(x = TrendStrength, y = SeasonalStrength, color = as.factor(Cluster))) +\n  geom_point(size = 3) +\n  labs(title = \"Clusters of Subcategories Based on Time-Series Features\",\n       x = \"Trend Strength\", y = \"Seasonal Strength\", color = \"Cluster\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nCode\n#\n#check\n#residual diagnostic\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n  for (subcategory in names(cluster_results)) {\n    cat(\"\\nResidual Diagnostics for Sub-Category:\", subcategory, \"\\n\")\n    checkresiduals(cluster_results[[subcategory]]$ARIMA$Model)\n  }\n}\n\n\n\nResidual Diagnostics for Sub-Category: Binders \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,2)(0,1,0)[12]\nQ* = 2.6295, df = 5, p-value = 0.7569\n\nModel df: 2.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Paper \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,1,1)(0,1,0)[12]\nQ* = 6.6676, df = 6, p-value = 0.3527\n\nModel df: 1.   Total lags used: 7\n\n\nResidual Diagnostics for Sub-Category: Furnishings \n\n\n\n\n\n\n\n\n\n\n    Ljung-Box test\n\ndata:  Residuals from ARIMA(0,0,0)(0,1,0)[12] with drift\nQ* = 9.5952, df = 7, p-value = 0.2127\n\nModel df: 0.   Total lags used: 7\n\n\nCode\n# P-value is higher then 0.1 so we have stationary data, this is good\n#cluster level metrics\ncluster_metrics &lt;- data.frame()\nfor (cluster_id in names(forecast_results_by_cluster)) {\n  cluster_results &lt;- forecast_results_by_cluster[[cluster_id]]\n  cluster_rmse &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"RMSE\"])\n  cluster_mape &lt;- sapply(cluster_results, function(x) x$ARIMA$Accuracy[\"Test set\", \"MAPE\"])\n  cluster_metrics &lt;- rbind(cluster_metrics, data.frame(Cluster = cluster_id, MeanRMSE = mean(cluster_rmse), MeanMAPE = mean(cluster_mape)))\n}\nprint(cluster_metrics)\n\n\n    Cluster  MeanRMSE MeanMAPE\n1 Cluster_1 10.783528 17.32927\n2 Cluster_2 12.230774 31.89519\n3 Cluster_3  7.782677 22.75340\n\n\nCluster 1 (e.g., Binders): ARIMA outperformed other methods due to significant autocorrelation and trend components.\nCluster 2 (e.g., Furnishings): ETS was the most accurate method, effectively balancing trend and seasonality.\nCluster 3 (e.g., Paper): ETS also performed best, with ARIMA showing higher error rates due to variability in random components.\nResidual diagnostics were performed for all ARIMA models, confirming no significant autocorrelation (p &gt; 0.05).\nCluster-Level Metrics based on mean RMSE and MAPE show: - Cluster 1 had the lowest RMSE using ARIMA. - Cluster 2 and 3 were better modeled with ETS\n\n\n3.4.4 Conclusion (4b)\nClustering allows for tailored forecasting strategies. We conclude that for the given data set ARIMA is more effective for clusters with strong trends, while ETS is preferable for clusters with mixed seasonal and trend characteristics. The approach aligns with lecture notes, emphasizing the importance of adapting models based on time series characteristics.",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html#forecasting-future-values",
    "href": "projects/scda.html#forecasting-future-values",
    "title": "Supply Chain Data Analytics",
    "section": "3.5 Forecasting future values",
    "text": "3.5 Forecasting future values\n\n3.5.1 Forecasting 3 products (5a)\nIn this session, we focused on evaluating different forecasting models (ARIMA, Holt-Winters, and ETS) for multiple sub-categories by analyzing their accuracy metrics, such as RMSE, MAPE, and residual diagnostics. Based on the evaluation results, we selected the best-performing model for each sub-category. We then used these models to forecast the future outcomes for each sub-category, projecting the data for the next year. Note: we may need to interpret the outcomes and explain why we pick the certain model\n\n\nCode\n#Binders-&gt;choose ARIMA\nbinders_ts &lt;- ts_list[[\"Binders\"]]\narima_model &lt;- auto.arima(binders_ts)\nsummary(arima_model)\n\n\nSeries: binders_ts \nARIMA(1,1,1)(0,1,0)[12] \n\nCoefficients:\n          ar1      ma1\n      -0.4781  -0.4819\ns.e.   0.2324   0.2426\n\nsigma^2 = 51.18:  log likelihood = -117.97\nAIC=241.94   AICc=242.72   BIC=246.61\n\nTraining set error measures:\n                   ME     RMSE      MAE       MPE     MAPE     MASE        ACF1\nTraining set 0.864453 5.931761 4.092168 -1.363101 15.06142 0.558023 -0.03746012\n\n\nCode\narima_forecast &lt;- forecast(arima_model, h = 12)\nprint(arima_forecast)\n\n\n         Point Forecast    Lo 80     Hi 80     Lo 95     Hi 95\nJan 2018       32.48390 23.31571  41.65208 18.462370  46.50543\nFeb 2018       23.77181 14.59632  32.94731  9.739103  37.80452\nMar 2018       45.85265 35.59989  56.10541 30.172412  61.53289\nApr 2018       46.20475 35.63662  56.77287 30.042185  62.36730\nMay 2018       44.08009 32.93968  55.22051 27.042295  61.11789\nJun 2018       44.61784 33.06359  56.17210 26.947139  62.28855\nJul 2018       40.36072 28.34867  52.37277 21.989869  58.73157\nAug 2018       44.48366 32.05796  56.90937 25.480189  63.48714\nSep 2018       73.42488 60.58630  86.26346 53.789962  93.05979\nOct 2018       51.45299 38.22024  64.68573 31.215253  71.69072\nNov 2018       72.43955 58.82134  86.05775 51.612293  93.26680\nDec 2018       89.44597 75.45417 103.43777 68.047363 110.84458\n\n\nCode\nplot(arima_forecast, main = \"ARIMA Forecast for Binders (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\nCode\n#Paper-&gt;choose ETS\npaper_ts &lt;- ts_list[[\"Paper\"]]\nets_model &lt;- ets(paper_ts)\nsummary(ets_model)\n\n\nETS(M,N,A) \n\nCall:\nets(y = paper_ts)\n\n  Smoothing parameters:\n    alpha = 0.3075 \n    gamma = 1e-04 \n\n  Initial states:\n    l = 22.5954 \n    s = 17.4472 16.5763 -4.1253 15.2986 0.421 -5.102\n           -0.6145 -0.0341 -7.985 -2.6766 -15.6576 -13.5481\n\n  sigma:  0.2365\n\n     AIC     AICc      BIC \n365.1517 380.1517 393.2197 \n\nTraining set error measures:\n                   ME     RMSE      MAE     MPE     MAPE      MASE       ACF1\nTraining set 1.450303 6.386648 4.166875 1.75373 14.03399 0.5245018 0.03600045\n\n\nCode\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\n\n\n         Point Forecast    Lo 80    Hi 80    Lo 95    Hi 95\nJan 2018       30.45588 21.22661 39.68516 16.34092 44.57085\nFeb 2018       28.34651 19.27484 37.41819 14.47258 42.22044\nMar 2018       41.32776 28.18367 54.47185 21.22561 61.42990\nApr 2018       36.01933 23.73891 48.29976 17.23804 54.80062\nMay 2018       43.97017 29.09477 58.84557 21.22021 66.72013\nJun 2018       43.39031 28.07408 58.70653 19.96616 66.81445\nJul 2018       38.90220 24.12853 53.67588 16.30782 61.49659\nAug 2018       44.42410 27.84663 61.00158 19.07104 69.77717\nSep 2018       59.30522 38.44478 80.16566 27.40193 91.20850\nOct 2018       39.87917 22.81850 56.93984 13.78712 65.97122\nNov 2018       60.58102 38.27853 82.88352 26.47230 94.68975\nDec 2018       61.45110 38.17748 84.72473 25.85717 97.04504\n\n\nCode\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\nCode\n#Furnishings-&gt;choose ETS\nfurnishings_ts &lt;- ts_list[[\"Furnishings\"]]\nets_model &lt;- ets(furnishings_ts)\nsummary(ets_model)\n\n\nETS(M,A,A) \n\nCall:\nets(y = furnishings_ts)\n\n  Smoothing parameters:\n    alpha = 0.0438 \n    beta  = 0.0437 \n    gamma = 2e-04 \n\n  Initial states:\n    l = 15.4275 \n    b = -0.1137 \n    s = 13.3158 15.6269 -2.2962 10.1503 -5.0017 -2.448\n           -3.4406 -1.0728 -1.1262 -4.3688 -11.689 -7.6497\n\n  sigma:  0.2527\n\n     AIC     AICc      BIC \n338.8888 359.2888 370.6992 \n\nTraining set error measures:\n                    ME     RMSE      MAE        MPE    MAPE      MASE\nTraining set 0.6402485 3.793384 2.884208 -0.8302416 16.2441 0.5352139\n                   ACF1\nTraining set 0.04613441\n\n\nCode\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\n\n\n         Point Forecast    Lo 80    Hi 80     Lo 95    Hi 95\nJan 2018       21.37433 14.45350 28.29515 10.789837 31.95882\nFeb 2018       18.56644 12.52244 24.61044  9.322944 27.80994\nMar 2018       27.11574 18.26943 35.96205 13.586481 40.64500\nApr 2018       31.58782 21.22159 41.95404 15.734048 47.44158\nMay 2018       32.87189 21.95559 43.78819 16.176845 49.56693\nJun 2018       31.73384 20.95052 42.51717 15.242170 48.22551\nJul 2018       33.95710 22.18459 45.72960 15.952604 51.96159\nAug 2018       32.63192 20.83816 44.42569 14.594916 50.66893\nSep 2018       49.01546 31.92061 66.11032 22.871140 75.15979\nOct 2018       37.79884 23.38308 52.21460 15.751844 59.84584\nNov 2018       56.95159 36.44698 77.45621 25.592490 88.31070\nDec 2018       55.87232 34.96477 76.77986 23.896983 87.84765\n\n\nCode\nplot(ets_forecast, main = \"ETS Forecast for Furnishings (Next 12 Months)\")\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 Applying to all data (5b)\nIn this session, we first grouped the sub-categories into clusters based on key time-series features, including trend strength, seasonal strength, and random strength, using hierarchical clustering. Once the clusters were formed, we applied and evaluated multiple forecasting models—ARIMA, Holt-Winters, and ETS—on each sub-category within its respective cluster, comparing their accuracy metrics such as RMSE and MAPE. Based on the evaluation results, we selected the best-performing model for each sub-category and used it to forecast the future outcomes within a year, leveraging the clustering to enhance the accuracy and relevance of our predictions.\n\n\nCode\n#Cluster_Binders-&gt;Holt-Winters\ncluster_id &lt;- 1\nsubcategory &lt;- \"Binders\"\nhw_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast &lt;- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Binders (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_paper-&gt;ETS\ncluster_id &lt;- 2\nsubcategory &lt;- \"Paper\"\nets_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$ETS$Model\nets_forecast &lt;- forecast(ets_model, h = 12)\nprint(ets_forecast)\nplot(ets_forecast, main = \"ETS Forecast for Paper (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")\n\n#Cluster_Furnishings-&gt;Holt-Winters\ncluster_id &lt;- 3\nsubcategory &lt;- \"Furnishings\"\nhw_model &lt;- forecast_results_by_cluster[[paste0(\"Cluster_\", cluster_id)]][[subcategory]]$HoltWinters$Model\nhw_forecast &lt;- forecast(hw_model, h = 12)\nprint(hw_forecast)\nplot(hw_forecast, main = \"Holt-Winters Forecast for Furnishings (Next 12 Months)\", xlab = \"Time\", ylab = \"Forecasted Values\")",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/scda.html#forecast-interpretation",
    "href": "projects/scda.html#forecast-interpretation",
    "title": "Supply Chain Data Analytics",
    "section": "3.6 Forecast interpretation",
    "text": "3.6 Forecast interpretation\nLorem Ipsum\n\n\n\nCode\n# Check for missing values\nmissing_values &lt;- colSums(is.na(data))\nprint(missing_values)  # Print missing values for reference\n# heat map\nlibrary(Amelia)\nmissmap(data, main = \"Missing Data Pattern\")\n#distribution of key variables\n#plot Quantity\nggplot(data, aes(x = Quantity)) +\n  geom_histogram(binwidth = 1, fill = \"steelblue\") +\n  labs(title = \"Distribution of Quantity\", x = \"Quantity\", y = \"Frequency\") +\n  theme_minimal()\n#plot sales\nggplot(data, aes(x = Sales)) +\n  geom_histogram(binwidth = 50, fill = \"tomato\") +\n  labs(title = \"Distribution of Sales\", x = \"Sales\", y = \"Frequency\") +\n  theme_minimal()\n# plot profit\nggplot(data, aes(x = Profit)) +\n  geom_histogram(binwidth = 10, fill = \"purple\") +\n  labs(title = \"Distribution of Profit\", x = \"Profit\", y = \"Frequency\") +\n  theme_minimal()\n# time based trends\ndata$Order_Date &lt;- as.Date(data$Order_Date, format = \"%Y-%m-%d\")  # Ensure date format\ntime_series &lt;- data %&gt;%\n  group_by(Order_Date) %&gt;%\n  summarize(total_sales = sum(Sales), total_profit = sum(Profit), total_quantity = sum(Quantity))\n\nggplot(time_series, aes(x = Order_Date)) +\n  geom_line(aes(y = total_sales, color = \"Sales\")) +\n  geom_line(aes(y = total_profit, color = \"Profit\")) +\n  geom_line(aes(y = total_quantity, color = \"Quantity\")) +\n  labs(title = \"Sales, Profit, and Quantity Over Time\", x = \"Date\", y = \"Value\") +\n  theme_minimal() +\n  scale_color_manual(name = \"Metrics\", values = c(\"Sales\" = \"blue\", \"Profit\" = \"green\", \"Quantity\" = \"red\"))\n\n#sales by category and sub category\ncategory_sales &lt;- data %&gt;%\n  group_by(Category, Sub_Category) %&gt;%\n  summarize(total_sales = sum(Sales))\n\nggplot(category_sales, aes(x = reorder(Sub_Category, -total_sales), y = total_sales, fill = Category)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"Sales by Category and Sub-Category\", x = \"Sub-Category\", y = \"Total Sales\") +\n  theme_minimal() +\n  coord_flip()\n\n#Outliers detection\n#Quantity\nggplot(data, aes(x = Category, y = Quantity)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Quantity by Category\", x = \"Category\", y = \"Quantity\")\n#sales\nggplot(data, aes(x = Category, y = Sales)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Sales by Category\", x = \"Category\", y = \"Sales\")\n\n#profit\nggplot(data, aes(x = Category, y = Profit)) +\n  geom_boxplot() +\n  labs(title = \"Boxplot of Profit by Category\", x = \"Category\", y = \"Profit\")\n#Geo visualization\n\nus_map &lt;- map_data(\"state\")\nif(\"State\" %in% colnames(data)) {\n  state_sales &lt;- data %&gt;%\n    group_by(State) %&gt;%\n    summarize(total_sales = sum(Sales))\n\n  # Convert state names to lowercase to match map data\n  state_sales$State &lt;- tolower(state_sales$State)\n\n  # Merge state sales data with map data\n  state_sales_map &lt;- merge(us_map, state_sales, by.x = \"region\", by.y = \"State\", all.x = TRUE)\n\n  # Plot sales by state\n  ggplot(state_sales_map, aes(long, lat, group = group, fill = total_sales)) +\n    geom_polygon(color = \"white\") +\n    scale_fill_continuous(low = \"lightblue\", high = \"darkblue\", na.value = \"gray90\") +\n    labs(title = \"Sales by State\", fill = \"Total Sales\") +\n    theme_void() +\n    coord_fixed(1.3)\n}\n\n# correlation matrix\nnumerical_data &lt;- data %&gt;% select(where(is.numeric))\n\ncor_matrix &lt;- cor(numerical_data, use = \"complete.obs\")\n\n# Convert the correlation matrix to a long format\ncor_data &lt;- as.data.frame(as.table(cor_matrix))\n\n# Plot the correlation matrix using ggplot2\nggplot(cor_data, aes(Var1, Var2, fill = Freq)) +\n  geom_tile(color = \"white\") +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\",\n                       midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n                       name=\"Correlation\") +\n  geom_text(aes(label = round(Freq, 2)), color = \"black\", size = 4) +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1,\n                                   size = 12, hjust = 1)) +\n  coord_fixed() +\n  labs(title = \"Correlation Matrix of Key Variables\", x = \"\", y = \"\")\n\n\n\n3.6.1 Forecasting??\n\n\nCode\n#Aggregate sales per month\nmonthly_sales &lt;- data %&gt;%\n  mutate(Month = floor_date(Order_Date, \"month\")) %&gt;%\n  group_by(Month) %&gt;%\n  summarize(total_sales = sum(Sales))\n#Convert to time series\nsales_ts &lt;- ts(monthly_sales$total_sales, frequency = 12, start = c(year(min(monthly_sales$Month)), month(min(monthly_sales$Month))))\n#Arima model\narima_model &lt;- auto.arima(sales_ts)\narima_forecast &lt;- forecast(arima_model, h = 12)\nautoplot(arima_forecast) + labs(title = \"ARIMA Forecast for Monthly Sales\")\n\n\n\n\n\n\n\n\n\nCode\n#Holts winter model\nhw_model &lt;- HoltWinters(sales_ts)\nhw_forecast &lt;- forecast(hw_model, h = 12)\nautoplot(hw_forecast) + labs(title = \"Holt-Winters Forecast for Monthly Sales\")\n\n\n\n\n\n\n\n\n\nCode\n# clustering for segmentation\nlibrary(cluster)\n#data clustering\nclustering_data &lt;- data %&gt;%\n  select(Sales, Quantity, Discount, Profit) %&gt;%\n  na.omit()\nset.seed(123)\nkmeans_model &lt;- kmeans(clustering_data, centers = 3)\ndata$Cluster &lt;- as.factor(kmeans_model$cluster)\n# visualize clustering result\nggplot(data, aes(x = Sales, y = Profit, color = Cluster)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"K-Means Clustering of Sales and Profit\", x = \"Sales\", y = \"Profit\") +\n  theme_minimal()",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Supply Chain Data Analytics paper</span>"
    ]
  },
  {
    "objectID": "projects/Rflashcards.html",
    "href": "projects/Rflashcards.html",
    "title": "Authorize and read the data",
    "section": "",
    "text": "#R Flashcards\nA simple script to pracitice with flashcards from within R.\nDuring my final year of Human Movement Sciences, we had an extensive anatomy course, and I requred some practice.\n\n\nCode\nrm(list = ls())\nlibrary(googlesheets4)\nlibrary(rstudioapi)\n\ngs4_deauth()\ndata &lt;- gs4_get(\"https://docs.google.com/spreadsheets/d/1iOpwVZc68hzQblAhpmvVtXU9a_Hq5VW6A1WTyHk9ngM/edit?usp=sharing\")\ndata &lt;- range_read(data, sheet = \"Sheet1\")\n\n# Ensure that the column names are correct\n# For example: data &lt;- data.frame(Question = data$Question, Answer = data$Answer)\n\n# Practice function\npractice_flashcards &lt;- function(data) {\n  asked_indices &lt;- integer()\n  \n  # Loop until all flashcards are practiced\n  while (length(asked_indices) &lt; nrow(data)) {\n    index &lt;- sample(setdiff(1:nrow(data), asked_indices), 1)\n    flashcard &lt;- data[index, ]  # Get the flashcard row\n    question &lt;- flashcard$Question\n    answer &lt;- as.character(flashcard$Answer)  # Ensure answer is character string\n    \n    # Prompt\n    cat(\"Question: \", question, \"\\n\")\n    user_input &lt;- readline(prompt = \"Your answer: \")\n    \n    # Check answer\n    if (tolower(user_input) == tolower(answer)) {\n      asked_indices &lt;- c(asked_indices, index)  # Mark as asked\n      cat(\"Correct answer!\\n\")\n    } else {\n      cat(\"Wrong answer. The correct answer was: \", answer, \"\\n\")  # This should work now\n      show_answer &lt;- readline(prompt = \"Do you want to see the question again? (y/n): \")\n      if (tolower(show_answer) == \"y\") {\n        cat(\"The correct answer is: \", answer, \"\\n\")\n      }\n    }\n    \n    cat(\"Remaining flashcards to practice: \", (nrow(data) - length(asked_indices)), \"\\n\\n\")\n  }\n  \n  cat(\"All flashcards have been practiced!\\n\")\n}\n\n\n# Main loop\nwhile (TRUE) {\n  practice_flashcards(data)\n  cat(\"Would you like to practice again? (y/n): \")\n  repeat_input &lt;- readline()\n  if (tolower(repeat_input) != \"y\") {\n    break\n  }\n}",
    "crumbs": [
      "VU University",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Flashcards for Anatomy</span>"
    ]
  }
]